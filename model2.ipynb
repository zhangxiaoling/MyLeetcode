{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "5B04DE4ADEEB46708F3F0789B2994CDB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03F057D2475B40FE938AA13466690475",
    "mdEditEnable": false
   },
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cell_type": "code",
    "collapsed": false,
    "id": "6A5AA962DD1546AC8E97A3A9F1096890",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68FD2948A7D947A88DC3DA588ABDC96A",
    "mdEditEnable": false
   },
   "source": [
    "# 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "D11B4578CD0440F4A5826F17E129D1C3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_path = '/home/kesci/input/round28100/' \n",
    "train_df = pd.read_csv(common_path + 'train_round_2.csv')\n",
    "test_df = pd.read_csv(common_path + 'test_round_2_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "A56F826EC9B048038F27C0324FA5F6FF",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10087, 344)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "8DA55861EECF482AAAC88B1FA470A29B",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33000, 346)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "871452B51A2B4539841DA3904C01706C",
    "mdEditEnable": false
   },
   "source": [
    "# 多天启动特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "310DB816928744A48E5082E1E5160D81",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_time_gap(strs,parm):\r\n",
    "    time = strs.split(\":\")\r\n",
    "    time = list(set(time))\r\n",
    "    time = sorted(list(map(lambda x:int(x),time)))\r\n",
    "    time_gap = []\r\n",
    "    #用户只在当天活跃\r\n",
    "    if len(time) == 1: return -20\r\n",
    "\r\n",
    "    for index, value in enumerate(time):\r\n",
    "        if index <= len(time) - 2:\r\n",
    "            gap = abs(time[index] - time[index + 1])\r\n",
    "            time_gap.append(gap)\r\n",
    "\r\n",
    "    if parm == '1': return np.mean(time_gap)\r\n",
    "    elif parm == '2': return np.max(time_gap)\r\n",
    "    elif parm == '3': return np.min(time_gap)\r\n",
    "    elif parm == '4': return np.std(time_gap)\r\n",
    "    elif parm == '5': return stats.skew(time_gap)\r\n",
    "    elif parm == '6': return stats.kurtosis(time_gap)\r\n",
    "    \r\n",
    "def get_week(day):\r\n",
    "    day = int(day)\r\n",
    "    if day >= 1 and day <= 7: return 1\r\n",
    "    if day >= 8 and day <= 14: return 2\r\n",
    "    if day >= 15 and day <= 21: return 3\r\n",
    "    if day >= 22 and day <= 28: return 4\r\n",
    "    if day >= 28: return 5\r\n",
    "\r\n",
    "def cur_day_repeat_count(strs):\r\n",
    "    time = strs.split(\":\")\r\n",
    "    time = dict(Counter(time))\r\n",
    "    time = sorted(time.items(), key=lambda x: x[1], reverse=False)\r\n",
    "    # 一天一次启动\r\n",
    "    if (len(time) == 1) & (time[0][1] == 1): return 0\r\n",
    "    # 一天多次启动\r\n",
    "    elif (len(time) == 1) & (time[0][1] > 1): return 1\r\n",
    "    # 多天多次启动\r\n",
    "    elif (len(time) > 1) & (time[0][1] >= 2): return 2\r\n",
    "    else: return 3\r\n",
    "\r\n",
    "def get_continue_day(day_list):\r\n",
    "    time = day_list.split(\":\")\r\n",
    "    time = list(map(lambda x:int(x),time))\r\n",
    "    m = np.array(time)\r\n",
    "    if len(set(m)) == 1:\r\n",
    "        return -1\r\n",
    "    m = list(set(m))\r\n",
    "    if len(m) == 0:\r\n",
    "        return -20\r\n",
    "    n = np.where(np.diff(m) == 1)[0]\r\n",
    "    i = 0\r\n",
    "    result = []\r\n",
    "    while i < len(n) - 1:\r\n",
    "        state = 1\r\n",
    "        while n[i + 1] - n[i] == 1:\r\n",
    "            state += 1\r\n",
    "            i += 1\r\n",
    "            if i == len(n) - 1:\r\n",
    "                break\r\n",
    "        if state == 1:\r\n",
    "            i += 1\r\n",
    "            result.append(2)\r\n",
    "        else:\r\n",
    "            i += 1\r\n",
    "            result.append(state + 1)\r\n",
    "    if len(n) == 1:\r\n",
    "        result.append(2)\r\n",
    "    if len(result) != 0:\r\n",
    "        # print(result)\r\n",
    "        return np.max(result)\r\n",
    "\r\n",
    "def get_continue_launch_count(strs,parm):\r\n",
    "    time = strs.split(\":\")\r\n",
    "    time = dict(Counter(time))\r\n",
    "    time = sorted(time.items(), key=lambda x: x[0], reverse=False)\r\n",
    "    key_list = []\r\n",
    "    value_list = []\r\n",
    "    if len(time) == 1:\r\n",
    "        return -2\r\n",
    "    for key,value in dict(time).items():\r\n",
    "        key_list.append(int(key))\r\n",
    "        value_list.append(int(value))\r\n",
    "\r\n",
    "    if np.mean(np.diff(key_list, 1)) == 1:\r\n",
    "        if parm == '1': return np.mean(value_list)\r\n",
    "        elif parm == '2': return np.max(value_list)\r\n",
    "        elif parm == '3': return np.min(value_list)\r\n",
    "        elif parm == '4': return np.sum(value_list)\r\n",
    "        elif parm == '5': return np.std(value_list)\r\n",
    "    else:\r\n",
    "        return -1\r\n",
    "\r\n",
    "def get_weekend(day):\r\n",
    "    day = int(day)\r\n",
    "    if day in [6,7,13,14,20,21,27,28]:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CFAD291EBDE4FE7B1B49A1356833BCB",
    "mdEditEnable": false
   },
   "source": [
    "# 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "id": "027AC5C3D5F5403EB51F80BF785D1353",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['mark'] = 'train'\r\n",
    "test_df['mark'] = 'test'\r\n",
    "total_df = train_df.append(test_df)\r\n",
    "\r\n",
    "# 简单加减特征\r\n",
    "total_df['seller_procduction'] = total_df['seller']+total_df['Product_id']\r\n",
    "total_df['user_procduction'] = total_df['user_id']+total_df['Product_id']\r\n",
    "total_df['seller_user'] = total_df['seller']+total_df['user_id']\r\n",
    "total_df['seller_user_procduction'] = total_df['seller']+total_df['user_id']+total_df['Product_id']\r\n",
    "# ?\r\n",
    "total_df['production_action'] = total_df['Product_id']+str(total_df['action_type'])\r\n",
    "\r\n",
    "feat1 = total_df.groupby(['seller_procduction'],as_index=False)['day'].agg({'s_p_count':'count'})\r\n",
    "total_df = pd.merge(total_df, feat1, on=['seller_procduction'], how='left')\r\n",
    "\r\n",
    "# feat1 = total_df.groupby(['user_procduction'],as_index=False)['day'].agg({'user_procduction_count':'count'})\r\n",
    "# total_df = pd.merge(total_df, feat1, on=['user_procduction'], how='left')\r\n",
    "\r\n",
    "feat1 = total_df.groupby(['seller_user'],as_index=False)['day'].agg({'seller_user_count':'count'})\r\n",
    "total_df = pd.merge(total_df, feat1, on=['seller_user'], how='left')\r\n",
    "\r\n",
    "# 用户是否多天有多次启动(均值)\r\n",
    "feat1 = total_df.groupby(['user_id'],as_index=False)['day'].agg({'user_count':'count'})\r\n",
    "feat3 = total_df[['user_id', 'day']]\r\n",
    "feat3['day'] = feat3['day'].astype('str')\r\n",
    "feat3 = feat3.groupby(['user_id'])['day'].agg(lambda x: ':'.join(x)).reset_index()\r\n",
    "feat3.rename(columns={'day': 'act_list'}, inplace=True)\r\n",
    "feat3['time_gap_mean'] = feat3['act_list'].apply(get_time_gap,args=('1'))\r\n",
    "# 最大\r\n",
    "feat3['time_gap_max'] = feat3['act_list'].apply(get_time_gap,args=('2'))\r\n",
    "# 最小\r\n",
    "feat3['time_gap_min'] = feat3['act_list'].apply(get_time_gap,args=('3'))\r\n",
    "# 方差\r\n",
    "feat3['time_gap_std'] = feat3['act_list'].apply(get_time_gap,args=('4'))\r\n",
    "# 锋度\r\n",
    "feat3['time_gap_skew'] = feat3['act_list'].apply(get_time_gap, args=('5'))\r\n",
    "# 偏度\r\n",
    "feat3['time_gap_kurt'] = feat3['act_list'].apply(get_time_gap, args=('6'))\r\n",
    "# 平均每天启动次数\r\n",
    "feat3['mean_act_count'] = feat3['act_list'].apply(lambda x: len(x.split(\":\")) / len(set(x.split(\":\"))))\r\n",
    "# 平均行为日期\r\n",
    "feat3['act_mean_date'] = \\\r\n",
    "feat3['act_list'].apply(lambda x: np.sum([int(ele) for ele in x.split(\":\")]) / len(x.split(\":\")))\r\n",
    "feat3['con_act_max'] = feat3['act_list'].apply(get_continue_day)\r\n",
    "del feat3['act_list']\r\n",
    "\r\n",
    "total_df = pd.merge(total_df, feat1, on=['user_id'], how='left')\r\n",
    "total_df = pd.merge(total_df, feat3, on=['user_id'], how='left')\r\n",
    "\r\n",
    "# 商品是否多天有多次启动(均值)\r\n",
    "feat1 = total_df.groupby(['Product_id'],as_index=False)['day'].agg({'Product_count':'count'})\r\n",
    "feat3 = total_df[['Product_id', 'day']]\r\n",
    "feat3['day'] = feat3['day'].astype('str')\r\n",
    "feat3 = feat3.groupby(['Product_id'])['day'].agg(lambda x: ':'.join(x)).reset_index()\r\n",
    "feat3.rename(columns={'day': 'act_list'}, inplace=True)\r\n",
    "feat3['time_gap_mean1'] = feat3['act_list'].apply(get_time_gap,args=('1'))\r\n",
    "# 最大\r\n",
    "feat3['time_gap_max1'] = feat3['act_list'].apply(get_time_gap,args=('2'))\r\n",
    "# 最小\r\n",
    "feat3['time_gap_min1'] = feat3['act_list'].apply(get_time_gap,args=('3'))\r\n",
    "# 方差\r\n",
    "feat3['time_gap_std1'] = feat3['act_list'].apply(get_time_gap,args=('4'))\r\n",
    "# 锋度\r\n",
    "feat3['time_gap_skew1'] = feat3['act_list'].apply(get_time_gap, args=('5'))\r\n",
    "# 偏度\r\n",
    "feat3['time_gap_kurt1'] = feat3['act_list'].apply(get_time_gap, args=('6'))\r\n",
    "# 平均每天启动次数\r\n",
    "feat3['mean_act_count1'] = feat3['act_list'].apply(lambda x: len(x.split(\":\")) / len(set(x.split(\":\"))))\r\n",
    "# 平均行为日期\r\n",
    "feat3['act_mean_date1'] = feat3['act_list'].apply(lambda x: np.sum([int(ele) for ele in x.split(\":\")]) / len(x.split(\":\")))\r\n",
    "\r\n",
    "feat3['con_act_max1'] = feat3['act_list'].apply(get_continue_day)\r\n",
    "del feat3['act_list']\r\n",
    "\r\n",
    "total_df = pd.merge(total_df, feat1, on=['Product_id'], how='left')\r\n",
    "total_df = pd.merge(total_df, feat3, on=['Product_id'], how='left')\r\n",
    "\r\n",
    "#卖家是否多天有多次启动(均值)\r\n",
    "feat1 = total_df.groupby(['seller'],as_index=False)['day'].agg({'seller_count':'count'})\r\n",
    "feat3 = total_df[['seller', 'day']]\r\n",
    "feat3['day'] = feat3['day'].astype('str')\r\n",
    "feat3 = feat3.groupby(['seller'])['day'].agg(lambda x: ':'.join(x)).reset_index()\r\n",
    "feat3.rename(columns={'day': 'act_list'}, inplace=True)\r\n",
    "feat3['time_gap_mean2'] = feat3['act_list'].apply(get_time_gap,args=('1'))\r\n",
    "# 最大\r\n",
    "feat3['time_gap_max2'] = feat3['act_list'].apply(get_time_gap,args=('2'))\r\n",
    "# 最小\r\n",
    "feat3['time_gap_min2'] = feat3['act_list'].apply(get_time_gap,args=('3'))\r\n",
    "# 方差\r\n",
    "feat3['time_gap_std2'] = feat3['act_list'].apply(get_time_gap,args=('4'))\r\n",
    "# 锋度\r\n",
    "feat3['time_gap_skew2'] = feat3['act_list'].apply(get_time_gap, args=('5'))\r\n",
    "# 偏度\r\n",
    "feat3['time_gap_kurt2'] = feat3['act_list'].apply(get_time_gap, args=('6'))\r\n",
    "# 平均每天启动次数\r\n",
    "feat3['mean_act_count2'] = feat3['act_list'].apply(lambda x: len(x.split(\":\")) / len(set(x.split(\":\"))))\r\n",
    "# 平均行为日期\r\n",
    "feat3['act_mean_date2'] = feat3['act_list'].apply(lambda x: np.sum([int(ele) for ele in x.split(\":\")]) / len(x.split(\":\")))\r\n",
    "\r\n",
    "feat3['con_act_max2'] = feat3['act_list'].apply(get_continue_day)\r\n",
    "del feat3['act_list']\r\n",
    "\r\n",
    "total_df = pd.merge(total_df, feat1, on=['seller'], how='left')\r\n",
    "total_df = pd.merge(total_df, feat3, on=['seller'], how='left')\r\n",
    "\r\n",
    "total_df['week'] = total_df['day'].apply(get_week)\r\n",
    "temp = pd.get_dummies(total_df['week'], prefix = 'week')\r\n",
    "total_df['week_new'] = list(map(lambda x: 'week_new' + str(x), total_df.week))\r\n",
    "temp = pd.crosstab(total_df.user_id,total_df.week_new).reset_index()\r\n",
    "del total_df['week_new']\r\n",
    "total_df = pd.merge(total_df, temp, on=['user_id'], how='left')\r\n",
    "del total_df['week']\r\n",
    "\r\n",
    "# *********************************************************************************\r\n",
    "#时间差分\r\n",
    "def timeFeatures(dataset):\r\n",
    "    dataset.sort_values(by = ['day'], ascending = True, inplace = True)\r\n",
    "    dataset['shift'] = dataset.groupby(['user_id'])['day'].apply(lambda x:x.shift(1))\r\n",
    "    dataset['user_span'] = dataset['day'] - dataset['shift']\r\n",
    "    # dataset['user_span'] =  dataset['user_span'].fillna(0).astype('int')\r\n",
    "    dataset.sort_index(axis = 0, inplace = True)\r\n",
    "    del dataset['shift']\r\n",
    "    \r\n",
    "    return dataset\r\n",
    "\r\n",
    "def action_feat(strs,parm):\r\n",
    "    action = strs.split(\":\")\r\n",
    "    count0 = action.count(\"0\")\r\n",
    "    count1 = action.count(\"1\")\r\n",
    "    count2 = action.count(\"2\")\r\n",
    "    count3 = action.count(\"3\")\r\n",
    "    count5 = action.count(\"5\")\r\n",
    "    if parm == 999: return len(action)\r\n",
    "    if parm == 0: return count0/len(action)\r\n",
    "    if parm == 1: return count1/len(action)\r\n",
    "    if parm == 2: return count2/len(action)\r\n",
    "    if parm == 3: return count3/len(action)\r\n",
    "    if parm == 5: return count5/len(action)\r\n",
    "\r\n",
    "# day相关\r\n",
    "total_df['product_start'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(min))\r\n",
    "total_df['product_end'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(max))\r\n",
    "# total_df['product_skew'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(stats.skew))\r\n",
    "# total_df['product_kurtosis'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(stats.kurtosis))\r\n",
    "# total_df['product_std'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(stats.kurtosis))\r\n",
    "# total_df['product_mean'] = total_df['product_end']-total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].agg(\"mean\"))\r\n",
    "total_df['seller_start'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(min))\r\n",
    "total_df['seller_end'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(max))\r\n",
    "# total_df['seller_skew'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(stats.skew))\r\n",
    "# total_df['seller_kurtosis'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(stats.kurtosis))\r\n",
    "# total_df['seller_std'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(\"std\"))\r\n",
    "# total_df['seller_mean'] = total_df['seller_end']-total_df['seller'].map(total_df.groupby(['seller'])['day'].agg(\"mean\"))\r\n",
    "total_df['user_start'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(min))\r\n",
    "total_df['user_end'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(max))\r\n",
    "# total_df['user_skew'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(stats.skew))\r\n",
    "# total_df['user_kurtosis'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(stats.kurtosis))\r\n",
    "# total_df['user_std'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(\"std\"))\r\n",
    "# total_df['user_mean'] = total_df['user_end']-total_df['user_id'].map(total_df.groupby(['user_id'])['day'].agg(\"mean\"))\r\n",
    "# total_df['product_in'] = total_df['product_end'] - total_df['product_start']\r\n",
    "# total_df['seller_in'] = total_df['seller_end'] - total_df['seller_start']\r\n",
    "# total_df['user_in'] = total_df['user_end'] - total_df['user_start']\r\n",
    "\r\n",
    "total_df['user_from_start'] = total_df['day'] - total_df['user_start']\r\n",
    "total_df['user_to_end'] = total_df['user_end'] - total_df['day']\r\n",
    "total_df['seller_from_start'] = total_df['day'] - total_df['seller_start']\r\n",
    "total_df['seller_to_end'] = total_df['seller_end'] - total_df['day']\r\n",
    "total_df['Product_id_from_start'] = total_df['day'] - total_df['product_start']\r\n",
    "total_df['Product_id_to_end'] = total_df['product_end'] - total_df['day']\r\n",
    "# total_df['user_to_mean'] = total_df['day'] - total_df['act_mean_date']\r\n",
    "# total_df['Product_to_mean'] = total_df['day'] - total_df['act_mean_date1']\r\n",
    "# total_df['seller_to_mean'] = total_df['day'] - total_df['act_mean_date2']\r\n",
    "\r\n",
    "# action_type\r\n",
    "# action_type_0 = train_df[train_df['action_type'] == 0]['favorite'].mean()\r\n",
    "# action_type_1 = train_df[train_df['action_type'] == 1]['favorite'].mean()\r\n",
    "# action_type_2 = train_df[train_df['action_type'] == 2]['favorite'].mean()\r\n",
    "# action_type_3 = train_df[train_df['action_type'] == 3]['favorite'].mean()\r\n",
    "# action_type_5 = train_df[train_df['action_type'] == 5]['favorite'].mean()\r\n",
    "# print(action_type_0,action_type_1,action_type_2,action_type_3,action_type_5)\r\n",
    "# total_df['action_type_'] = total_df['action_type'].map({2:action_type_2, 3:action_type_3, 0:action_type_0, 1:action_type_1, 5:action_type_5})\r\n",
    "\r\n",
    "# total_df['action_type'] = total_df['action_type'].map({5:1, 2:2, 0:3, 1:4, 4:5})\r\n",
    "# action_type = pd.get_dummies(total_df['action_type'], prefix = 'action_type')\r\n",
    "# total_df = total_df.join(action_type)\r\n",
    "\r\n",
    "# feat1 = total_df[['user_id', 'action_type']]\r\n",
    "# feat1['action_type'] = feat1['action_type'].astype('str')\r\n",
    "# feat1 = feat1.groupby(['user_id'])['action_type'].agg(lambda x: ':'.join(x)).reset_index(name='action_list')\r\n",
    "    \r\n",
    "# feat1['count'] = feat1['action_list'].apply(lambda x: action_feat(x,999))\r\n",
    "# feat1['action_0_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,0))\r\n",
    "# feat1['action_1_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,1))\r\n",
    "# feat1['action_2_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,2))\r\n",
    "# feat1['action_3_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,3))\r\n",
    "# feat1['action_5_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,5))\r\n",
    "# del feat1['count']\r\n",
    "# del feat1['action_list']\r\n",
    "# total_df = pd.merge(total_df,feat1,on = ['user_id'], how ='left')\r\n",
    "# del feat1\r\n",
    "\r\n",
    "# feat1 = total_df[['Product_id', 'action_type']]\r\n",
    "# feat1['action_type'] = feat1['action_type'].astype('str')\r\n",
    "# feat1 = feat1.groupby(['Product_id'])['action_type'].agg(lambda x: ':'.join(x)).reset_index(name='action_list')\r\n",
    "    \r\n",
    "# feat1['count'] = feat1['action_list'].apply(lambda x: action_feat(x,999))\r\n",
    "# feat1['action_0_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,0))\r\n",
    "# feat1['action_1_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,1))\r\n",
    "# feat1['action_2_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,2))\r\n",
    "# feat1['action_3_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,3))\r\n",
    "# feat1['action_5_rate'] = feat1['action_list'].apply(lambda x: action_feat(x,5))\r\n",
    "# del feat1['count']\r\n",
    "# del feat1['action_list']\r\n",
    "# total_df = pd.merge(total_df,feat1,on = ['Product_id'], how ='left')\r\n",
    "# del feat1\r\n",
    "\r\n",
    "# webInfo\r\n",
    "# total_df['user_web_11'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_1'].agg(\"mean\"))\r\n",
    "# total_df['user_web_12'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_1'].agg(\"max\"))\r\n",
    "# total_df['user_web_13'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_1'].agg(\"min\"))\r\n",
    "# total_df['user_web_14'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_1'].agg(\"std\"))\r\n",
    "# total_df['user_web_21'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_2'].agg(\"mean\"))\r\n",
    "# total_df['user_web_22'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_2'].agg(\"max\"))\r\n",
    "# total_df['user_web_23'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_2'].agg(\"min\"))\r\n",
    "# total_df['user_web_24'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_2'].agg(\"std\"))\r\n",
    "# total_df['user_web_31'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_3'].agg(\"mean\"))\r\n",
    "# total_df['user_web_32'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_3'].agg(\"max\"))\r\n",
    "# total_df['user_web_33'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_3'].agg(\"min\"))\r\n",
    "# total_df['user_web_34'] = total_df['user_id'].map(total_df.groupby(['user_id'])['WebInfo_3'].agg(\"std\"))\r\n",
    "# total_df['WebInfo_123'] = total_df['WebInfo_1'] + total_df['WebInfo_2'] + total_df['WebInfo_3']\r\n",
    "# total_df['WebInfo_12'] = total_df['WebInfo_1'] + total_df['WebInfo_2']\r\n",
    "# total_df['WebInfo_23'] = total_df['WebInfo_2'] + total_df['WebInfo_3']\r\n",
    "# total_df['WebInfo_13'] = total_df['WebInfo_1'] + total_df['WebInfo_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "BA0C3664A9894CA3818C5B2E3680A6CB",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43087, 401)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 389 + 6 + 6 + 13(12) = 414 \n",
    "temp_df = total_df.copy()\n",
    "total_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06DC102BC33B4AA3891801B5E7338B2D",
    "mdEditEnable": false
   },
   "source": [
    "# 特征2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "id": "72ACD70753E44DFF97F51716BC82E491",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33000, 428)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = temp_df.copy()\r\n",
    "\r\n",
    "# total_df['weekend'] = total_df['day'].apply(get_weekend)\r\n",
    "\r\n",
    "# 两两组合\r\n",
    "# total_df['user_day'] = total_df['user_id'].map(total_df.groupby(['user_id'])['day'].nunique())\r\n",
    "# total_df['day_user'] = total_df['day'].map(total_df.groupby(['day'])['user_id'].nunique())\r\n",
    "# total_df['seller_day'] = total_df['seller'].map(total_df.groupby(['seller'])['day'].nunique())\r\n",
    "# total_df['day_seller'] = total_df['day'].map(total_df.groupby(['day'])['seller'].nunique())\r\n",
    "# total_df['product_day'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['day'].nunique())\r\n",
    "# total_df['day_product'] = total_df['day'].map(total_df.groupby(['day'])['Product_id'].nunique())\r\n",
    "total_df['seller_user_'] = total_df['seller'].map(total_df.groupby(['seller'])['user_id'].nunique())\r\n",
    "total_df['user_seller'] = total_df['user_id'].map(total_df.groupby(['user_id'])['seller'].nunique())\r\n",
    "total_df['seller_product'] = total_df['seller'].map(total_df.groupby(['seller'])['Product_id'].nunique())\r\n",
    "total_df['product_seller'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['seller'].nunique())\r\n",
    "total_df['product_user'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['user_id'].nunique())\r\n",
    "total_df['user_product'] = total_df['user_id'].map(total_df.groupby(['user_id'])['Product_id'].nunique())\r\n",
    "# total_df['product_action'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type'].nunique())\r\n",
    "# total_df['action_product'] = total_df['action_type'].map(total_df.groupby(['action_type'])['Product_id'].nunique())\r\n",
    "# total_df['seller_action'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type'].nunique())\r\n",
    "# total_df['action_seller'] = total_df['action_type'].map(total_df.groupby(['action_type'])['seller'].nunique())\r\n",
    "# total_df['user_action'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type'].nunique())\r\n",
    "# total_df['action_user'] = total_df['action_type'].map(total_df.groupby(['action_type'])['user_id'].nunique())\r\n",
    "\r\n",
    "total_df['action_type_'] = total_df['action_type'].map({2:1, 3:2, 0:3, 1:4, 5:5})\r\n",
    "total_df['user_action_1_'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type_'].agg(\"mean\"))\r\n",
    "total_df['user_action_2_'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type_'].agg(\"max\"))\r\n",
    "total_df['user_action_3_'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type_'].agg(\"min\"))\r\n",
    "total_df['user_action_4_'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type_'].agg(\"std\"))\r\n",
    "# total_df['user_action_5_'] = total_df['user_id'].map(total_df.groupby(['user_id'])['action_type_'].agg(\"sum\"))\r\n",
    "\r\n",
    "total_df['seller_action_1_'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type_'].agg(\"mean\"))\r\n",
    "total_df['seller_action_2_'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type_'].agg(\"max\"))\r\n",
    "total_df['seller_action_3_'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type_'].agg(\"min\"))\r\n",
    "total_df['seller_action_4_'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type_'].agg(\"std\"))\r\n",
    "# total_df['seller_action_5_'] = total_df['seller'].map(total_df.groupby(['seller'])['action_type_'].agg(\"sum\"))\r\n",
    "\r\n",
    "total_df['product_action_1_'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type_'].agg(\"mean\"))\r\n",
    "total_df['product_action_2_'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type_'].agg(\"max\"))\r\n",
    "total_df['product_action_3_'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type_'].agg(\"min\"))\r\n",
    "total_df['product_action_4_'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type_'].agg(\"std\"))\r\n",
    "# total_df['product_action_5_'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['action_type_'].agg(\"sum\"))\r\n",
    "\r\n",
    "\r\n",
    "# 特征对\r\n",
    "# user_seller = total_df.groupby(['user_id','seller']).size().reset_index(name='user_seller_d')\r\n",
    "# total_df = pd.merge(total_df,user_seller,on = ['user_id','seller'], how ='left')\r\n",
    "# user_seller = total_df.groupby(['user_id','Product_id']).size().reset_index(name='user_Product_d')\r\n",
    "# total_df = pd.merge(total_df,user_seller,on = ['user_id','Product_id'], how ='left')\r\n",
    "# user_seller = total_df.groupby(['Product_id','seller']).size().reset_index(name='Product_seller_d')\r\n",
    "# total_df = pd.merge(total_df,user_seller,on = ['Product_id','seller'], how ='left')\r\n",
    "# user_product_seller = total_df.groupby(['user_id','Product_id','seller']).size().reset_index(name='user_product_seller')\r\n",
    "# total_df = pd.merge(total_df,user_product_seller,on = ['user_id','Product_id','seller'], how ='left')\r\n",
    "\r\n",
    "# day为粒度\r\n",
    "user_per_day = total_df.groupby(['user_id','day']).size().reset_index(name='user_per_day')\r\n",
    "# user_per_day['user_per_day_min'] = \\\r\n",
    "# user_per_day['user_id'].map(user_per_day.groupby(['user_id'])['user_per_day'].agg('min'))\r\n",
    "# user_per_day['user_per_day_max'] = \\\r\n",
    "# user_per_day['user_id'].map(user_per_day.groupby(['user_id'])['user_per_day'].agg('max'))\r\n",
    "# user_per_day['user_per_day_std'] = \\\r\n",
    "# user_per_day['user_id'].map(user_per_day.groupby(['user_id'])['user_per_day'].agg('std'))\r\n",
    "\r\n",
    "seller_per_day = total_df.groupby(['seller','day']).size().reset_index(name='seller_per_day')\r\n",
    "# seller_per_day['seller_per_day_min'] = \\\r\n",
    "# seller_per_day['seller'].map(seller_per_day.groupby(['seller'])['seller_per_day'].agg('min'))\r\n",
    "# seller_per_day['seller_per_day_max'] = \\\r\n",
    "# seller_per_day['seller'].map(seller_per_day.groupby(['seller'])['seller_per_day'].agg('max'))\r\n",
    "# seller_per_day['seller_per_day_std'] = \\\r\n",
    "# seller_per_day['seller'].map(seller_per_day.groupby(['seller'])['seller_per_day'].agg('std'))\r\n",
    "\r\n",
    "product_per_day = total_df.groupby(['Product_id','day']).size().reset_index(name='product_per_day')\r\n",
    "# product_per_day['Produc_per_day_min'] = \\\r\n",
    "# product_per_day['Product_id'].map(product_per_day.groupby(['Product_id'])['product_per_day'].agg('min'))\r\n",
    "# product_per_day['Produc_per_day_max'] = \\\r\n",
    "# product_per_day['Product_id'].map(product_per_day.groupby(['Product_id'])['product_per_day'].agg('max'))\r\n",
    "# product_per_day['Product_per_day_std'] = \\\r\n",
    "# product_per_day['Product_id'].map(product_per_day.groupby(['Product_id'])['product_per_day'].agg('std'))\r\n",
    "total_df = pd.merge(total_df,user_per_day,on = ['user_id','day'], how ='left')\r\n",
    "total_df = pd.merge(total_df,seller_per_day,on = ['seller','day'], how ='left')\r\n",
    "total_df = pd.merge(total_df,product_per_day,on = ['Product_id','day'], how ='left')\r\n",
    "\r\n",
    "\r\n",
    "# 以week为粒度\r\n",
    "# total_df['week'] = total_df['day'].apply(get_week)\r\n",
    "# total_df['product_start_week'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['week'].agg(min))\r\n",
    "# total_df['product_end_week'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['week'].agg(max))\r\n",
    "# total_df['seller_start_week'] = total_df['seller'].map(total_df.groupby(['seller'])['week'].agg(min))\r\n",
    "# total_df['seller_end_week'] = total_df['seller'].map(total_df.groupby(['seller'])['week'].agg(max))\r\n",
    "# total_df['user_start_week'] = total_df['user_id'].map(total_df.groupby(['user_id'])['week'].agg(min))\r\n",
    "# total_df['user_end_week'] = total_df['user_id'].map(total_df.groupby(['user_id'])['week'].agg(max))\r\n",
    "\r\n",
    "# user_per_week = total_df.groupby(['user_id','week']).size().reset_index(name='user_per_week')\r\n",
    "# user_per_week['user_per_week_min'] = \\\r\n",
    "# user_per_week['user_id'].map(user_per_week.groupby(['user_id'])['user_per_week'].agg('min'))\r\n",
    "# user_per_week['user_per_week_max'] = \\\r\n",
    "# user_per_week['user_id'].map(user_per_week.groupby(['user_id'])['user_per_week'].agg('max'))\r\n",
    "# user_per_week['user_per_week_std'] = \\\r\n",
    "# user_per_week['user_id'].map(user_per_week.groupby(['user_id'])['user_per_week'].agg('std'))\r\n",
    "\r\n",
    "# seller_per_week = total_df.groupby(['seller','week']).size().reset_index(name='seller_per_week')\r\n",
    "# seller_per_week['seller_per_week_min'] = \\\r\n",
    "# seller_per_week['seller'].map(seller_per_week.groupby(['seller'])['seller_per_week'].agg('min'))\r\n",
    "# seller_per_week['seller_per_week_max'] = \\\r\n",
    "# seller_per_week['seller'].map(seller_per_week.groupby(['seller'])['seller_per_week'].agg('max'))\r\n",
    "# seller_per_week['seller_per_week_std'] = \\\r\n",
    "# seller_per_week['seller'].map(seller_per_week.groupby(['seller'])['seller_per_week'].agg('std'))\r\n",
    "\r\n",
    "# product_per_week = total_df.groupby(['Product_id','week']).size().reset_index(name='product_per_week')\r\n",
    "# product_per_week['product_per_week_min'] = \\\r\n",
    "# product_per_week['Product_id'].map(product_per_week.groupby(['Product_id'])['product_per_week'].agg('min'))\r\n",
    "# product_per_week['product_per_week_max'] = \\\r\n",
    "# product_per_week['Product_id'].map(product_per_week.groupby(['Product_id'])['product_per_week'].agg('max'))\r\n",
    "# product_per_week['product_per_week_std'] = \\\r\n",
    "# product_per_week['Product_id'].map(product_per_week.groupby(['Product_id'])['product_per_week'].agg('std'))\r\n",
    "\r\n",
    "# total_df = pd.merge(total_df,user_per_week,on = ['user_id','week'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,seller_per_week,on = ['seller','week'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,product_per_week,on = ['Product_id','week'], how ='left')\r\n",
    "\r\n",
    "# 星期几\r\n",
    "# total_df['workday'] = total_df['day'].apply(lambda x: x%7)\r\n",
    "# user_per_week = total_df.groupby(['user_id','workday']).size().reset_index(name='user_per_week')\r\n",
    "# seller_per_week = total_df.groupby(['seller','workday']).size().reset_index(name='seller_per_week')\r\n",
    "# product_per_week = total_df.groupby(['Product_id','workday']).size().reset_index(name='product_per_week')\r\n",
    "# total_df = pd.merge(total_df,user_per_week,on = ['user_id','workday'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,seller_per_week,on = ['seller','workday'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,product_per_week,on = ['Product_id','workday'], how ='left')\r\n",
    "\r\n",
    "# # [UserInfo_4,UserInfo_186,UserInfo_229,UserInfo_169,UserInfo_121]\r\n",
    "# total_df['user_4_1'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_4'].agg(\"mean\"))\r\n",
    "# total_df['user_4_2'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_4'].agg(\"max\"))\r\n",
    "# total_df['user_4_3'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_4'].agg(\"min\"))\r\n",
    "# total_df['user_4_4'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_4'].agg(\"std\"))\r\n",
    "# total_df['UserInfo_4_186'] = total_df['UserInfo_4'] + total_df['UserInfo_186']\r\n",
    "# total_df['UserInfo_186_229'] = total_df['UserInfo_186'] + total_df['UserInfo_229']\r\n",
    "# total_df['UserInfo_229_169'] = total_df['UserInfo_229'] + total_df['UserInfo_169']\r\n",
    "# total_df['UserInfo_169_121'] = total_df['UserInfo_169'] + total_df['UserInfo_121']\r\n",
    "\r\n",
    "#web\r\n",
    "# total_df['is_web3_56'] = total_df['WebInfo_3'].apply(lambda x: 1 if x == 0  else 0  )\r\n",
    "# total_df['user_web_11'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_1'].agg(\"mean\"))\r\n",
    "# total_df['user_web_12'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_1'].agg(\"max\"))\r\n",
    "# total_df['user_web_13'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_1'].agg(\"min\"))\r\n",
    "# total_df['user_web_14'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_1'].agg(\"std\"))\r\n",
    "# total_df['user_web_15'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_1'].nunique())\r\n",
    "# total_df['user_web_21'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_2'].agg(\"mean\"))\r\n",
    "# total_df['user_web_22'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_2'].agg(\"max\"))\r\n",
    "# total_df['user_web_23'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_2'].agg(\"min\"))\r\n",
    "# total_df['user_web_24'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_2'].agg(\"std\"))\r\n",
    "# total_df['user_web_25'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_2'].nunique())\r\n",
    "# total_df['user_web_31'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_3'].agg(\"mean\"))\r\n",
    "# total_df['user_web_32'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_3'].agg(\"max\"))\r\n",
    "# total_df['user_web_33'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_3'].agg(\"min\"))\r\n",
    "# total_df['user_web_34'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_3'].agg(\"std\"))\r\n",
    "# total_df['user_web_35'] = total_df['Product_id'].map(total_df.groupby(['Product_id'])['WebInfo_3'].nunique())\r\n",
    "\r\n",
    "# 权重\r\n",
    "# total_df['weight'] = total_df['day'].map(train_df.groupby(['day'])['favorite'].agg('mean'))\r\n",
    "\r\n",
    "# w2w_user_product = total_df.groupby(['user_id'])['Product_id'] \\\r\n",
    "#                     .agg(lambda x: ''.join(x)).reset_index(name='w2w_user_product')\r\n",
    "# total_df = pd.merge(total_df,w2w_user_product,on = ['user_id'], how ='left')\r\n",
    "# from gensim.models import word2vec\r\n",
    "# sentences = [[x] for x in total_df['w2w_user_product']]\r\n",
    "\r\n",
    "# model = word2vec.Word2Vec(sentences,size=5,min_count=1,)\r\n",
    "# w2w0,w2w1,w2w2,w2w3,w2w4 = [],[],[],[],[]\r\n",
    "\r\n",
    "# for i in range(len(sentences)):\r\n",
    "#     w2w0.append(model[sentences[i][0]][0])\r\n",
    "#     w2w1.append(model[sentences[i][0]][1])\r\n",
    "#     w2w2.append(model[sentences[i][0]][2])\r\n",
    "#     w2w3.append(model[sentences[i][0]][3])\r\n",
    "#     w2w4.append(model[sentences[i][0]][4])\r\n",
    "# total_df['w2w0'] = pd.DataFrame(w2w0)\r\n",
    "# total_df['w2w1'] = pd.DataFrame(w2w1)\r\n",
    "# total_df['w2w2'] = pd.DataFrame(w2w2)\r\n",
    "# total_df['w2w3'] = pd.DataFrame(w2w3)\r\n",
    "# total_df['w2w4'] = pd.DataFrame(w2w4)\r\n",
    "# del total_df['w2w_user_product']\r\n",
    "\r\n",
    "# time\r\n",
    "# total_df['start'] = total_df['user_start'] + total_df['seller_start'] + total_df['product_start']\r\n",
    "# total_df['end'] = total_df['user_end'] + total_df['seller_end'] + total_df['product_end']\r\n",
    "\r\n",
    "# total_df['test1'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_148'].agg('max'))\r\n",
    "# total_df['test2'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_148'].agg('min'))\r\n",
    "# total_df['test3'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_148'].agg('mean'))\r\n",
    "# total_df['test4'] = total_df['user_id'].map(total_df.groupby(['user_id'])['UserInfo_148'].agg('std'))\r\n",
    "\r\n",
    "# # cumcount\r\n",
    "# # total_df = temp_df.copy()\r\n",
    "# day_df = temp_df.copy()\r\n",
    "# day_df = total_df[['user_id','Product_id','seller','day']]\r\n",
    "# day_df.sort_values(\"day\",inplace=True)\r\n",
    "# day_df['user_cumcount'] = day_df.groupby('user_id').cumcount()\r\n",
    "# day_df['seller_cumcount'] = day_df.groupby('seller').cumcount()\r\n",
    "# day_df['Product_id_cumcount'] = day_df.groupby('Product_id').cumcount()\r\n",
    "# day_df['user_cumcount_'] = day_df[::-1].groupby('user_id').cumcount()\r\n",
    "# day_df['seller_cumcount_'] = day_df[::-1].groupby('seller').cumcount()\r\n",
    "# day_df['Product_id_cumcount_'] = day_df[::-1].groupby('Product_id').cumcount()\r\n",
    "# day_df.sort_index(inplace=True)\r\n",
    "# # day_df[day_df['user_id'] == 'dgijam'][['user_cumcount','user_id','day']]\r\n",
    "# # total_df = pd.merge(total_df,day_df, how ='left')\r\n",
    "# total_df['user_cumcount'] = day_df['user_cumcount']\r\n",
    "# total_df['seller_cumcount'] = day_df['seller_cumcount']\r\n",
    "# total_df['Product_id_cumcount'] = day_df['Product_id_cumcount']\r\n",
    "# total_df['user_cumcount_'] = day_df['user_cumcount_']\r\n",
    "# total_df['seller_cumcount_'] = day_df['seller_cumcount_']\r\n",
    "# total_df['Product_id_cumcount_'] = day_df['Product_id_cumcount_']\r\n",
    "# # total_df.shape\r\n",
    "# # day_df[day_df['user_id'] == 'dgijam'][['user_cumcount','user_id','day']]\r\n",
    "\r\n",
    "# 和前面重复\r\n",
    "# ten_days = total_df[(total_df.day >=20 ) & (total_df.day <= 30)]\r\n",
    "# user_size_10 = ten_days.groupby(['user_id']).size().reset_index(name='user_size_10')\r\n",
    "# seller_size_10 = ten_days.groupby(['seller']).size().reset_index(name='seller_size_10')\r\n",
    "# Product_id_size_10 = ten_days.groupby(['Product_id']).size().reset_index(name='Product_id_size_10')\r\n",
    "# total_df = pd.merge(total_df,user_size_10,on = ['user_id'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,seller_size_10,on = ['seller'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,Product_id_size_10,on = ['Product_id'], how ='left')\r\n",
    "\r\n",
    "user_size = total_df.groupby(['user_id']).size().reset_index(name='user_size')\r\n",
    "seller_size = total_df.groupby(['seller']).size().reset_index(name='seller_size')\r\n",
    "Product_id_size = total_df.groupby(['Product_id']).size().reset_index(name='Product_id_size')\r\n",
    "# seller_user_size = total_df.groupby(['seller_user']).size().reset_index(name='seller_user_size')\r\n",
    "total_df = pd.merge(total_df,user_size,on = ['user_id'], how ='left')\r\n",
    "total_df = pd.merge(total_df,seller_size,on = ['seller'], how ='left')\r\n",
    "total_df = pd.merge(total_df,Product_id_size,on = ['Product_id'], how ='left')\r\n",
    "# total_df = pd.merge(total_df,seller_user_size,on = ['seller_user'], how ='left')\r\n",
    "\r\n",
    "train_df = total_df[total_df['mark']=='train']\r\n",
    "test_df = total_df[total_df['mark']=='test']\r\n",
    "\r\n",
    "train_df['mean_favorite'] = train_df['user_id'].map(train_df.groupby(['user_id'])['favorite'].agg('mean'))\r\n",
    "train_df['mean(favorite)'] = (train_df['mean_favorite'] * train_df['user_size'] - train_df['favorite'])/(train_df['user_size']-1)\r\n",
    "data_dict = train_df.groupby('user_id')['favorite'].agg('mean').to_dict()\r\n",
    "test_df['mean(favorite)'] = test_df['user_id'].map(data_dict)\r\n",
    "\r\n",
    "train_df['mean_favorite_seller'] = train_df['seller'].map(train_df.groupby(['seller'])['favorite'].agg('mean'))\r\n",
    "train_df['mean(favorite)_seller'] = (train_df['mean_favorite_seller'] * train_df['seller_size'] - train_df['favorite'])/(train_df['seller_size']-1)\r\n",
    "data_dict = train_df.groupby('seller')['favorite'].agg('mean').to_dict()\r\n",
    "test_df['mean(favorite)_seller'] = test_df['seller'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['mean_favorite_seller_user'] = train_df['seller_user'].map(train_df.groupby(['seller_user'])['favorite'].agg('mean'))\r\n",
    "# train_df['mean(favorite)_seller_user'] = (train_df['mean_favorite_seller_user'] * train_df['seller_user_size'] \\\r\n",
    "# - train_df['favorite'])/(train_df['seller_user_size']-1)\r\n",
    "# data_dict = train_df.groupby('seller_user')['favorite'].agg('mean').to_dict()\r\n",
    "# test_df['mean(favorite)_seller_user'] = test_df['seller_user'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['mean_favorite_product'] = train_df['Product_id'].map(train_df.groupby(['Product_id'])['favorite'].agg('mean'))\r\n",
    "# train_df['mean(favorite)_product'] = (train_df['mean_favorite_product'] * train_df['Product_id_size'] - train_df['favorite'])/(train_df['Product_id_size']-1)\r\n",
    "# data_dict = train_df.groupby('Product_id')['favorite'].agg('mean').to_dict()\r\n",
    "# test_df['mean(favorite)_product'] = test_df['Product_id'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['mean_purchase'] = train_df['user_id'].map(train_df.groupby(['user_id'])['purchase'].agg('mean'))\r\n",
    "# train_df['mean(purchase)'] = (train_df['mean_purchase'] * train_df['user_size'] - train_df['purchase'])/(train_df['user_size']-1)\r\n",
    "# data_dict = train_df.groupby('user_id')['purchase'].agg('mean').to_dict()\r\n",
    "# test_df['mean(purchase)'] = test_df['user_id'].map(data_dict)\r\n",
    "\r\n",
    "train_df['mean_purchase_seller'] = train_df['seller'].map(train_df.groupby(['seller'])['purchase'].agg('mean'))\r\n",
    "train_df['mean(purchase)_seller'] = (train_df['mean_purchase_seller'] * train_df['seller_size'] - train_df['purchase'])/(train_df['seller_size']-1)\r\n",
    "data_dict = train_df.groupby('seller')['purchase'].agg('mean').to_dict()\r\n",
    "test_df['mean(purchase)_seller'] = test_df['seller'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['mean_purchase_product'] = train_df['Product_id'].map(train_df.groupby(['Product_id']) \\\r\n",
    "# ['purchase'].agg('mean'))\r\n",
    "# train_df['mean(purchase)_product'] = (train_df['mean_purchase_product'] * train_df['Product_id_size']  \\\r\n",
    "# - train_df['purchase'])/(train_df['Product_id_size']-1)\r\n",
    "# data_dict = train_df.groupby('Product_id')['purchase'].agg('mean').to_dict()\r\n",
    "# test_df['mean(purchase)_product'] = test_df['Product_id'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['mean(favorite)_day'] =  train_df['mean(favorite)'] * train_df['user_start']\r\n",
    "# test_df['mean(favorite)_day'] =  test_df['mean(favorite)'] * test_df['user_start']\r\n",
    "# train_df['mean(favorite)_seller_day'] =  train_df['mean(favorite)_seller'] * train_df['seller_start']\r\n",
    "# test_df['mean(favorite)_seller_day'] =  test_df['mean(favorite)_seller'] * test_df['seller_start']\r\n",
    "# train_df['mean(purchase)_seller_day'] =  train_df['mean(purchase)_seller'] * train_df['product_start']\r\n",
    "# test_df['mean(purchase)_seller_day'] =  test_df['mean(purchase)_seller'] * test_df['product_start']\r\n",
    "\r\n",
    "# woe\r\n",
    "# favorite_df = train_df[train_df['favorite'] == 1]\r\n",
    "# train_df['events'] = train_df['user_id'].map(favorite_df.groupby(['user_id']).size())\r\n",
    "# non_favorite_df = train_df[train_df['favorite'] == 0]\r\n",
    "# train_df['non_events'] = train_df['user_id'].map(non_favorite_df.groupby(['user_id']).size())\r\n",
    "# train_df['non_events'] = train_df['non_events'].fillna(0)\r\n",
    "# train_df['events'] = train_df['events'].fillna(0)\r\n",
    "# import math\r\n",
    "# train_df['woe'] = ((train_df['non_events'] + 0.5)/len(non_favorite_df))/ \\\r\n",
    "# ((train_df['events'] + 0.5)/len(favorite_df))\r\n",
    "# train_df['woe'] = train_df['woe'].apply(lambda x: math.log(x))\r\n",
    "# data_dict = train_df.groupby('user_id')['woe'].agg('mean').to_dict()\r\n",
    "# test_df['woe'] = test_df['user_id'].map(data_dict)\r\n",
    "\r\n",
    "# train_df['events_rate'] = train_df['events']/len(favorite_df)\r\n",
    "# train_df['non_events_rate'] = train_df['non_events']/len(non_favorite_df)\r\n",
    "# train_df['IV'] = (train_df['events_rate'] - train_df['non_events_rate'])*train_df['woe']\r\n",
    "\r\n",
    "# del train_df['events']\r\n",
    "# del train_df['non_events']\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "del train_df['mark']\r\n",
    "del test_df['mark']\r\n",
    "del train_df['mean_favorite']\r\n",
    "del train_df['mean_favorite_seller']\r\n",
    "# del train_df['mean_favorite_seller_user']\r\n",
    "\r\n",
    "# del train_df['mean_purchase']\r\n",
    "del train_df['mean_purchase_seller']\r\n",
    "# del train_df['mean_purchase_product']\r\n",
    "train_df.shape\r\n",
    "# 394 406 412 418 419 422 423 424 425 428 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "id": "C91509521867414DBBCEDFD895510C9D",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10087, 430)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "id": "393A391EF8E14E0992DF139F2F1EA612",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def count_batplot(feature):\r\n",
    "#     f, ax =plt.subplots(4,1, figsize = (20,12))\r\n",
    "#     sns.countplot(feature, hue = 'favorite', data = train_df, ax = ax[0])\r\n",
    "#     sns.countplot(feature, hue = 'purchase', data = train_df, ax = ax[2])\r\n",
    "#     sns.barplot(x = feature, y = 'favorite', data = train_df, palette=\"Blues_d\", ax = ax[1])\r\n",
    "#     sns.barplot(x = feature, y = 'purchase', data = train_df, palette=\"Blues_d\", ax = ax[3])\r\n",
    "#     plt.show()\r\n",
    "# count_batplot('start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "id": "9B1072AAC3854B3C850FC4B23304D7AD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,f_regression\r\n",
    "import xgboost as xgb\r\n",
    "def xgb_fea_select(train_x, train_y, f_name_list):\r\n",
    "    rate_fea = 0.5\r\n",
    "    dtrain=xgb.DMatrix(train_x, label=train_y)\r\n",
    "    params = {\r\n",
    "        'booster':'gbtree',\r\n",
    "              'max_depth': 3,\r\n",
    "              'colsample_bytree': 0.7,\r\n",
    "              'subsample': 0.7, \r\n",
    "              'eta': 0.03,\r\n",
    "              'silent': 1,\r\n",
    "              # 'objective': 'binary:logistic',\r\n",
    "             'objective': 'rank:pairwise',\r\n",
    "              'min_child_weight': 3,\r\n",
    "              'seed': 10,\r\n",
    "              'eval_metric':'auc',\r\n",
    "            #   'scale_pos_weight': 17410 / 15590,\r\n",
    "            #   'scale_pos_weight': 3176 / 76824,\r\n",
    "              'scale_pos_weight': 4062/28938,\r\n",
    "              'verbose': 100,\r\n",
    "        \r\n",
    "    }\r\n",
    "    watchlist = [(dtrain,'train')]\r\n",
    "    bst=xgb.train(params,dtrain,num_boost_round=500,evals=watchlist, \r\n",
    "                  early_stopping_rounds=100)\r\n",
    "    fscore_dict = bst.get_fscore()\r\n",
    "    # print(fscore_dict)\r\n",
    "    sorted_fs_dict = sorted(fscore_dict.items(),key = lambda x:x[1],reverse = True)\r\n",
    "    # print([sorted_fs_dict[0] for x in sorted_fs_dict])\r\n",
    "    fea_id_set = ([(item[0][:]) for item in sorted_fs_dict[:int(len(sorted_fs_dict)*rate_fea)]])\r\n",
    "    # print(fea_id_set)\r\n",
    "    # f_name_list = [item for i, item in enumerate(f_name_list) if i in fea_id_set]\r\n",
    "    return fea_id_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F76B1EF1E2148A68232F63616129DAB",
    "mdEditEnable": false
   },
   "source": [
    "# 筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "id": "D6EE7B55B1FB4464B37C9FD255712A39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.69749\n",
      "Will train until train-auc hasn't improved in 100 rounds.\n",
      "[1]\ttrain-auc:0.717612\n",
      "[2]\ttrain-auc:0.734915\n",
      "[3]\ttrain-auc:0.739087\n",
      "[4]\ttrain-auc:0.745396\n",
      "[5]\ttrain-auc:0.746121\n",
      "[6]\ttrain-auc:0.750536\n",
      "[7]\ttrain-auc:0.747089\n",
      "[8]\ttrain-auc:0.750077\n",
      "[9]\ttrain-auc:0.751562\n",
      "[10]\ttrain-auc:0.75096\n",
      "[11]\ttrain-auc:0.753935\n",
      "[12]\ttrain-auc:0.755902\n",
      "[13]\ttrain-auc:0.756468\n",
      "[14]\ttrain-auc:0.756725\n",
      "[15]\ttrain-auc:0.758002\n",
      "[16]\ttrain-auc:0.758003\n",
      "[17]\ttrain-auc:0.757325\n",
      "[18]\ttrain-auc:0.756567\n",
      "[19]\ttrain-auc:0.758243\n",
      "[20]\ttrain-auc:0.759094\n",
      "[21]\ttrain-auc:0.759327\n",
      "[22]\ttrain-auc:0.758621\n",
      "[23]\ttrain-auc:0.757921\n",
      "[24]\ttrain-auc:0.758209\n",
      "[25]\ttrain-auc:0.758273\n",
      "[26]\ttrain-auc:0.757704\n",
      "[27]\ttrain-auc:0.757346\n",
      "[28]\ttrain-auc:0.757601\n",
      "[29]\ttrain-auc:0.757463\n",
      "[30]\ttrain-auc:0.757069\n",
      "[31]\ttrain-auc:0.756411\n",
      "[32]\ttrain-auc:0.757184\n",
      "[33]\ttrain-auc:0.757082\n",
      "[34]\ttrain-auc:0.758947\n",
      "[35]\ttrain-auc:0.759287\n",
      "[36]\ttrain-auc:0.758656\n",
      "[37]\ttrain-auc:0.758354\n",
      "[38]\ttrain-auc:0.758098\n",
      "[39]\ttrain-auc:0.758188\n",
      "[40]\ttrain-auc:0.757761\n",
      "[41]\ttrain-auc:0.757464\n",
      "[42]\ttrain-auc:0.757758\n",
      "[43]\ttrain-auc:0.758369\n",
      "[44]\ttrain-auc:0.758615\n",
      "[45]\ttrain-auc:0.759302\n",
      "[46]\ttrain-auc:0.759051\n",
      "[47]\ttrain-auc:0.76007\n",
      "[48]\ttrain-auc:0.761058\n",
      "[49]\ttrain-auc:0.761537\n",
      "[50]\ttrain-auc:0.761102\n",
      "[51]\ttrain-auc:0.761822\n",
      "[52]\ttrain-auc:0.76198\n",
      "[53]\ttrain-auc:0.761593\n",
      "[54]\ttrain-auc:0.761732\n",
      "[55]\ttrain-auc:0.761536\n",
      "[56]\ttrain-auc:0.761697\n",
      "[57]\ttrain-auc:0.761326\n",
      "[58]\ttrain-auc:0.761184\n",
      "[59]\ttrain-auc:0.761249\n",
      "[60]\ttrain-auc:0.76134\n",
      "[61]\ttrain-auc:0.761314\n",
      "[62]\ttrain-auc:0.761096\n",
      "[63]\ttrain-auc:0.761505\n",
      "[64]\ttrain-auc:0.761871\n",
      "[65]\ttrain-auc:0.762242\n",
      "[66]\ttrain-auc:0.762588\n",
      "[67]\ttrain-auc:0.762711\n",
      "[68]\ttrain-auc:0.763351\n",
      "[69]\ttrain-auc:0.763164\n",
      "[70]\ttrain-auc:0.763244\n",
      "[71]\ttrain-auc:0.763933\n",
      "[72]\ttrain-auc:0.764334\n",
      "[73]\ttrain-auc:0.764265\n",
      "[74]\ttrain-auc:0.764469\n",
      "[75]\ttrain-auc:0.764853\n",
      "[76]\ttrain-auc:0.765186\n",
      "[77]\ttrain-auc:0.765264\n",
      "[78]\ttrain-auc:0.765991\n",
      "[79]\ttrain-auc:0.765784\n",
      "[80]\ttrain-auc:0.766657\n",
      "[81]\ttrain-auc:0.76672\n",
      "[82]\ttrain-auc:0.766746\n",
      "[83]\ttrain-auc:0.76704\n",
      "[84]\ttrain-auc:0.767308\n",
      "[85]\ttrain-auc:0.767641\n",
      "[86]\ttrain-auc:0.768259\n",
      "[87]\ttrain-auc:0.768095\n",
      "[88]\ttrain-auc:0.76838\n",
      "[89]\ttrain-auc:0.769163\n",
      "[90]\ttrain-auc:0.769187\n",
      "[91]\ttrain-auc:0.768927\n",
      "[92]\ttrain-auc:0.768943\n",
      "[93]\ttrain-auc:0.768919\n",
      "[94]\ttrain-auc:0.769095\n",
      "[95]\ttrain-auc:0.769253\n",
      "[96]\ttrain-auc:0.76927\n",
      "[97]\ttrain-auc:0.769504\n",
      "[98]\ttrain-auc:0.770012\n",
      "[99]\ttrain-auc:0.770026\n",
      "[100]\ttrain-auc:0.770652\n",
      "[101]\ttrain-auc:0.771192\n",
      "[102]\ttrain-auc:0.771054\n",
      "[103]\ttrain-auc:0.771643\n",
      "[104]\ttrain-auc:0.771459\n",
      "[105]\ttrain-auc:0.771497\n",
      "[106]\ttrain-auc:0.771325\n",
      "[107]\ttrain-auc:0.771376\n",
      "[108]\ttrain-auc:0.771669\n",
      "[109]\ttrain-auc:0.771734\n",
      "[110]\ttrain-auc:0.772285\n",
      "[111]\ttrain-auc:0.772209\n",
      "[112]\ttrain-auc:0.772241\n",
      "[113]\ttrain-auc:0.772413\n",
      "[114]\ttrain-auc:0.772888\n",
      "[115]\ttrain-auc:0.772864\n",
      "[116]\ttrain-auc:0.773274\n",
      "[117]\ttrain-auc:0.773423\n",
      "[118]\ttrain-auc:0.773904\n",
      "[119]\ttrain-auc:0.77431\n",
      "[120]\ttrain-auc:0.774687\n",
      "[121]\ttrain-auc:0.774611\n",
      "[122]\ttrain-auc:0.774566\n",
      "[123]\ttrain-auc:0.774654\n",
      "[124]\ttrain-auc:0.774991\n",
      "[125]\ttrain-auc:0.775136\n",
      "[126]\ttrain-auc:0.77529\n",
      "[127]\ttrain-auc:0.775522\n",
      "[128]\ttrain-auc:0.775614\n",
      "[129]\ttrain-auc:0.775914\n",
      "[130]\ttrain-auc:0.776172\n",
      "[131]\ttrain-auc:0.776173\n",
      "[132]\ttrain-auc:0.776568\n",
      "[133]\ttrain-auc:0.77648\n",
      "[134]\ttrain-auc:0.776727\n",
      "[135]\ttrain-auc:0.776812\n",
      "[136]\ttrain-auc:0.776875\n",
      "[137]\ttrain-auc:0.777235\n",
      "[138]\ttrain-auc:0.777668\n",
      "[139]\ttrain-auc:0.778059\n",
      "[140]\ttrain-auc:0.778446\n",
      "[141]\ttrain-auc:0.778443\n",
      "[142]\ttrain-auc:0.778631\n",
      "[143]\ttrain-auc:0.77904\n",
      "[144]\ttrain-auc:0.778988\n",
      "[145]\ttrain-auc:0.779062\n",
      "[146]\ttrain-auc:0.779354\n",
      "[147]\ttrain-auc:0.779413\n",
      "[148]\ttrain-auc:0.77976\n",
      "[149]\ttrain-auc:0.779987\n",
      "[150]\ttrain-auc:0.780426\n",
      "[151]\ttrain-auc:0.780805\n",
      "[152]\ttrain-auc:0.781109\n",
      "[153]\ttrain-auc:0.781314\n",
      "[154]\ttrain-auc:0.781632\n",
      "[155]\ttrain-auc:0.781952\n",
      "[156]\ttrain-auc:0.782108\n",
      "[157]\ttrain-auc:0.782106\n",
      "[158]\ttrain-auc:0.782334\n",
      "[159]\ttrain-auc:0.782658\n",
      "[160]\ttrain-auc:0.782638\n",
      "[161]\ttrain-auc:0.782843\n",
      "[162]\ttrain-auc:0.782974\n",
      "[163]\ttrain-auc:0.783232\n",
      "[164]\ttrain-auc:0.78352\n",
      "[165]\ttrain-auc:0.783723\n",
      "[166]\ttrain-auc:0.783811\n",
      "[167]\ttrain-auc:0.783787\n",
      "[168]\ttrain-auc:0.783889\n",
      "[169]\ttrain-auc:0.783795\n",
      "[170]\ttrain-auc:0.783909\n",
      "[171]\ttrain-auc:0.784139\n",
      "[172]\ttrain-auc:0.784367\n",
      "[173]\ttrain-auc:0.784541\n",
      "[174]\ttrain-auc:0.784797\n",
      "[175]\ttrain-auc:0.785039\n",
      "[176]\ttrain-auc:0.785373\n",
      "[177]\ttrain-auc:0.785374\n",
      "[178]\ttrain-auc:0.785537\n",
      "[179]\ttrain-auc:0.785794\n",
      "[180]\ttrain-auc:0.786081\n",
      "[181]\ttrain-auc:0.786255\n",
      "[182]\ttrain-auc:0.786495\n",
      "[183]\ttrain-auc:0.786765\n",
      "[184]\ttrain-auc:0.787041\n",
      "[185]\ttrain-auc:0.787152\n",
      "[186]\ttrain-auc:0.787432\n",
      "[187]\ttrain-auc:0.787572\n",
      "[188]\ttrain-auc:0.78775\n",
      "[189]\ttrain-auc:0.787864\n",
      "[190]\ttrain-auc:0.787872\n",
      "[191]\ttrain-auc:0.788014\n",
      "[192]\ttrain-auc:0.78815\n",
      "[193]\ttrain-auc:0.788335\n",
      "[194]\ttrain-auc:0.788537\n",
      "[195]\ttrain-auc:0.788725\n",
      "[196]\ttrain-auc:0.789089\n",
      "[197]\ttrain-auc:0.789213\n",
      "[198]\ttrain-auc:0.789431\n",
      "[199]\ttrain-auc:0.789624\n",
      "[200]\ttrain-auc:0.789663\n",
      "[201]\ttrain-auc:0.789649\n",
      "[202]\ttrain-auc:0.789749\n",
      "[203]\ttrain-auc:0.789945\n",
      "[204]\ttrain-auc:0.789958\n",
      "[205]\ttrain-auc:0.79021\n",
      "[206]\ttrain-auc:0.790352\n",
      "[207]\ttrain-auc:0.790469\n",
      "[208]\ttrain-auc:0.790422\n",
      "[209]\ttrain-auc:0.790625\n",
      "[210]\ttrain-auc:0.790874\n",
      "[211]\ttrain-auc:0.791073\n",
      "[212]\ttrain-auc:0.791352\n",
      "[213]\ttrain-auc:0.791528\n",
      "[214]\ttrain-auc:0.791844\n",
      "[215]\ttrain-auc:0.792049\n",
      "[216]\ttrain-auc:0.792255\n",
      "[217]\ttrain-auc:0.792366\n",
      "[218]\ttrain-auc:0.79254\n",
      "[219]\ttrain-auc:0.792643\n",
      "[220]\ttrain-auc:0.792901\n",
      "[221]\ttrain-auc:0.793031\n",
      "[222]\ttrain-auc:0.793049\n",
      "[223]\ttrain-auc:0.793139\n",
      "[224]\ttrain-auc:0.793453\n",
      "[225]\ttrain-auc:0.793559\n",
      "[226]\ttrain-auc:0.79375\n",
      "[227]\ttrain-auc:0.793948\n",
      "[228]\ttrain-auc:0.794163\n",
      "[229]\ttrain-auc:0.794313\n",
      "[230]\ttrain-auc:0.794494\n",
      "[231]\ttrain-auc:0.794501\n",
      "[232]\ttrain-auc:0.794536\n",
      "[233]\ttrain-auc:0.794633\n",
      "[234]\ttrain-auc:0.794683\n",
      "[235]\ttrain-auc:0.794851\n",
      "[236]\ttrain-auc:0.794932\n",
      "[237]\ttrain-auc:0.795069\n",
      "[238]\ttrain-auc:0.795227\n",
      "[239]\ttrain-auc:0.795376\n",
      "[240]\ttrain-auc:0.795477\n",
      "[241]\ttrain-auc:0.795608\n",
      "[242]\ttrain-auc:0.795867\n",
      "[243]\ttrain-auc:0.796103\n",
      "[244]\ttrain-auc:0.796332\n",
      "[245]\ttrain-auc:0.796356\n",
      "[246]\ttrain-auc:0.796384\n",
      "[247]\ttrain-auc:0.796586\n",
      "[248]\ttrain-auc:0.796706\n",
      "[249]\ttrain-auc:0.796918\n",
      "[250]\ttrain-auc:0.797101\n",
      "[251]\ttrain-auc:0.797226\n",
      "[252]\ttrain-auc:0.797457\n",
      "[253]\ttrain-auc:0.797673\n",
      "[254]\ttrain-auc:0.797816\n",
      "[255]\ttrain-auc:0.798005\n",
      "[256]\ttrain-auc:0.797997\n",
      "[257]\ttrain-auc:0.798169\n",
      "[258]\ttrain-auc:0.798377\n",
      "[259]\ttrain-auc:0.798362\n",
      "[260]\ttrain-auc:0.798615\n",
      "[261]\ttrain-auc:0.798766\n",
      "[262]\ttrain-auc:0.798859\n",
      "[263]\ttrain-auc:0.799044\n",
      "[264]\ttrain-auc:0.799212\n",
      "[265]\ttrain-auc:0.799375\n",
      "[266]\ttrain-auc:0.799528\n",
      "[267]\ttrain-auc:0.799528\n",
      "[268]\ttrain-auc:0.799672\n",
      "[269]\ttrain-auc:0.799831\n",
      "[270]\ttrain-auc:0.799955\n",
      "[271]\ttrain-auc:0.800091\n",
      "[272]\ttrain-auc:0.800178\n",
      "[273]\ttrain-auc:0.800388\n",
      "[274]\ttrain-auc:0.800428\n",
      "[275]\ttrain-auc:0.800393\n",
      "[276]\ttrain-auc:0.800542\n",
      "[277]\ttrain-auc:0.80066\n",
      "[278]\ttrain-auc:0.800741\n",
      "[279]\ttrain-auc:0.800932\n",
      "[280]\ttrain-auc:0.801089\n",
      "[281]\ttrain-auc:0.801239\n",
      "[282]\ttrain-auc:0.801289\n",
      "[283]\ttrain-auc:0.801339\n",
      "[284]\ttrain-auc:0.801464\n",
      "[285]\ttrain-auc:0.801753\n",
      "[286]\ttrain-auc:0.801816\n",
      "[287]\ttrain-auc:0.801857\n",
      "[288]\ttrain-auc:0.801861\n",
      "[289]\ttrain-auc:0.80203\n",
      "[290]\ttrain-auc:0.80218\n",
      "[291]\ttrain-auc:0.802221\n",
      "[292]\ttrain-auc:0.802399\n",
      "[293]\ttrain-auc:0.802526\n",
      "[294]\ttrain-auc:0.802712\n",
      "[295]\ttrain-auc:0.802759\n",
      "[296]\ttrain-auc:0.802908\n",
      "[297]\ttrain-auc:0.802994\n",
      "[298]\ttrain-auc:0.803191\n",
      "[299]\ttrain-auc:0.803314\n",
      "[300]\ttrain-auc:0.803425\n",
      "[301]\ttrain-auc:0.803409\n",
      "[302]\ttrain-auc:0.803531\n",
      "[303]\ttrain-auc:0.803681\n",
      "[304]\ttrain-auc:0.803832\n",
      "[305]\ttrain-auc:0.804004\n",
      "[306]\ttrain-auc:0.804155\n",
      "[307]\ttrain-auc:0.804379\n",
      "[308]\ttrain-auc:0.80456\n",
      "[309]\ttrain-auc:0.80456\n",
      "[310]\ttrain-auc:0.804777\n",
      "[311]\ttrain-auc:0.804944\n",
      "[312]\ttrain-auc:0.805053\n",
      "[313]\ttrain-auc:0.805179\n",
      "[314]\ttrain-auc:0.805246\n",
      "[315]\ttrain-auc:0.8054\n",
      "[316]\ttrain-auc:0.805507\n",
      "[317]\ttrain-auc:0.805677\n",
      "[318]\ttrain-auc:0.805799\n",
      "[319]\ttrain-auc:0.805831\n",
      "[320]\ttrain-auc:0.805967\n",
      "[321]\ttrain-auc:0.806046\n",
      "[322]\ttrain-auc:0.806217\n",
      "[323]\ttrain-auc:0.806374\n",
      "[324]\ttrain-auc:0.80653\n",
      "[325]\ttrain-auc:0.806659\n",
      "[326]\ttrain-auc:0.806691\n",
      "[327]\ttrain-auc:0.806756\n",
      "[328]\ttrain-auc:0.806878\n",
      "[329]\ttrain-auc:0.807011\n",
      "[330]\ttrain-auc:0.807139\n",
      "[331]\ttrain-auc:0.807303\n",
      "[332]\ttrain-auc:0.807425\n",
      "[333]\ttrain-auc:0.807485\n",
      "[334]\ttrain-auc:0.807624\n",
      "[335]\ttrain-auc:0.807761\n",
      "[336]\ttrain-auc:0.807884\n",
      "[337]\ttrain-auc:0.808011\n",
      "[338]\ttrain-auc:0.808144\n",
      "[339]\ttrain-auc:0.808312\n",
      "[340]\ttrain-auc:0.808398\n",
      "[341]\ttrain-auc:0.80846\n",
      "[342]\ttrain-auc:0.808593\n",
      "[343]\ttrain-auc:0.808724\n",
      "[344]\ttrain-auc:0.80886\n",
      "[345]\ttrain-auc:0.808964\n",
      "[346]\ttrain-auc:0.809104\n",
      "[347]\ttrain-auc:0.80928\n",
      "[348]\ttrain-auc:0.80933\n",
      "[349]\ttrain-auc:0.809466\n",
      "[350]\ttrain-auc:0.809573\n",
      "[351]\ttrain-auc:0.809674\n",
      "[352]\ttrain-auc:0.809757\n",
      "[353]\ttrain-auc:0.809837\n",
      "[354]\ttrain-auc:0.809993\n",
      "[355]\ttrain-auc:0.810137\n",
      "[356]\ttrain-auc:0.810255\n",
      "[357]\ttrain-auc:0.81039\n",
      "[358]\ttrain-auc:0.810371\n",
      "[359]\ttrain-auc:0.810409\n",
      "[360]\ttrain-auc:0.810493\n",
      "[361]\ttrain-auc:0.810627\n",
      "[362]\ttrain-auc:0.810786\n",
      "[363]\ttrain-auc:0.810879\n",
      "[364]\ttrain-auc:0.811008\n",
      "[365]\ttrain-auc:0.811107\n",
      "[366]\ttrain-auc:0.811264\n",
      "[367]\ttrain-auc:0.8113\n",
      "[368]\ttrain-auc:0.811416\n",
      "[369]\ttrain-auc:0.811604\n",
      "[370]\ttrain-auc:0.811696\n",
      "[371]\ttrain-auc:0.811812\n",
      "[372]\ttrain-auc:0.811937\n",
      "[373]\ttrain-auc:0.812101\n",
      "[374]\ttrain-auc:0.8122\n",
      "[375]\ttrain-auc:0.812279\n",
      "[376]\ttrain-auc:0.812388\n",
      "[377]\ttrain-auc:0.81247\n",
      "[378]\ttrain-auc:0.812603\n",
      "[379]\ttrain-auc:0.812595\n",
      "[380]\ttrain-auc:0.812701\n",
      "[381]\ttrain-auc:0.812784\n",
      "[382]\ttrain-auc:0.812915\n",
      "[383]\ttrain-auc:0.813018\n",
      "[384]\ttrain-auc:0.813117\n",
      "[385]\ttrain-auc:0.813254\n",
      "[386]\ttrain-auc:0.813359\n",
      "[387]\ttrain-auc:0.813491\n",
      "[388]\ttrain-auc:0.813571\n",
      "[389]\ttrain-auc:0.81365\n",
      "[390]\ttrain-auc:0.81376\n",
      "[391]\ttrain-auc:0.81381\n",
      "[392]\ttrain-auc:0.813938\n",
      "[393]\ttrain-auc:0.814033\n",
      "[394]\ttrain-auc:0.81404\n",
      "[395]\ttrain-auc:0.814127\n",
      "[396]\ttrain-auc:0.814194\n",
      "[397]\ttrain-auc:0.814338\n",
      "[398]\ttrain-auc:0.814497\n",
      "[399]\ttrain-auc:0.814596\n",
      "[400]\ttrain-auc:0.814641\n",
      "[401]\ttrain-auc:0.814684\n",
      "[402]\ttrain-auc:0.814772\n",
      "[403]\ttrain-auc:0.814939\n",
      "[404]\ttrain-auc:0.815023\n",
      "[405]\ttrain-auc:0.815061\n",
      "[406]\ttrain-auc:0.815146\n",
      "[407]\ttrain-auc:0.815263\n",
      "[408]\ttrain-auc:0.815414\n",
      "[409]\ttrain-auc:0.815522\n",
      "[410]\ttrain-auc:0.81562\n",
      "[411]\ttrain-auc:0.815707\n",
      "[412]\ttrain-auc:0.815704\n",
      "[413]\ttrain-auc:0.81582\n",
      "[414]\ttrain-auc:0.815861\n",
      "[415]\ttrain-auc:0.815952\n",
      "[416]\ttrain-auc:0.816039\n",
      "[417]\ttrain-auc:0.816185\n",
      "[418]\ttrain-auc:0.816268\n",
      "[419]\ttrain-auc:0.816364\n",
      "[420]\ttrain-auc:0.816483\n",
      "[421]\ttrain-auc:0.816616\n",
      "[422]\ttrain-auc:0.816727\n",
      "[423]\ttrain-auc:0.816869\n",
      "[424]\ttrain-auc:0.816915\n",
      "[425]\ttrain-auc:0.817016\n",
      "[426]\ttrain-auc:0.817095\n",
      "[427]\ttrain-auc:0.817251\n",
      "[428]\ttrain-auc:0.817358\n",
      "[429]\ttrain-auc:0.81746\n",
      "[430]\ttrain-auc:0.817462\n",
      "[431]\ttrain-auc:0.817604\n",
      "[432]\ttrain-auc:0.817739\n",
      "[433]\ttrain-auc:0.81784\n",
      "[434]\ttrain-auc:0.817971\n",
      "[435]\ttrain-auc:0.818074\n",
      "[436]\ttrain-auc:0.818204\n",
      "[437]\ttrain-auc:0.818167\n",
      "[438]\ttrain-auc:0.818289\n",
      "[439]\ttrain-auc:0.818407\n",
      "[440]\ttrain-auc:0.818505\n",
      "[441]\ttrain-auc:0.818495\n",
      "[442]\ttrain-auc:0.818605\n",
      "[443]\ttrain-auc:0.818681\n",
      "[444]\ttrain-auc:0.818771\n",
      "[445]\ttrain-auc:0.818858\n",
      "[446]\ttrain-auc:0.818936\n",
      "[447]\ttrain-auc:0.819056\n",
      "[448]\ttrain-auc:0.819227\n",
      "[449]\ttrain-auc:0.819328\n",
      "[450]\ttrain-auc:0.819443\n",
      "[451]\ttrain-auc:0.81957\n",
      "[452]\ttrain-auc:0.819643\n",
      "[453]\ttrain-auc:0.819761\n",
      "[454]\ttrain-auc:0.819837\n",
      "[455]\ttrain-auc:0.819943\n",
      "[456]\ttrain-auc:0.820001\n",
      "[457]\ttrain-auc:0.820137\n",
      "[458]\ttrain-auc:0.82021\n",
      "[459]\ttrain-auc:0.820271\n",
      "[460]\ttrain-auc:0.820444\n",
      "[461]\ttrain-auc:0.820515\n",
      "[462]\ttrain-auc:0.82057\n",
      "[463]\ttrain-auc:0.820617\n",
      "[464]\ttrain-auc:0.820694\n",
      "[465]\ttrain-auc:0.820774\n",
      "[466]\ttrain-auc:0.820866\n",
      "[467]\ttrain-auc:0.820948\n",
      "[468]\ttrain-auc:0.82105\n",
      "[469]\ttrain-auc:0.821147\n",
      "[470]\ttrain-auc:0.821262\n",
      "[471]\ttrain-auc:0.821277\n",
      "[472]\ttrain-auc:0.821399\n",
      "[473]\ttrain-auc:0.821476\n",
      "[474]\ttrain-auc:0.821573\n",
      "[475]\ttrain-auc:0.82172\n",
      "[476]\ttrain-auc:0.821786\n",
      "[477]\ttrain-auc:0.821917\n",
      "[478]\ttrain-auc:0.821981\n",
      "[479]\ttrain-auc:0.822086\n",
      "[480]\ttrain-auc:0.822094\n",
      "[481]\ttrain-auc:0.822159\n",
      "[482]\ttrain-auc:0.822245\n",
      "[483]\ttrain-auc:0.822343\n",
      "[484]\ttrain-auc:0.822346\n",
      "[485]\ttrain-auc:0.822425\n",
      "[486]\ttrain-auc:0.822526\n",
      "[487]\ttrain-auc:0.82263\n",
      "[488]\ttrain-auc:0.822757\n",
      "[489]\ttrain-auc:0.822828\n",
      "[490]\ttrain-auc:0.822927\n",
      "[491]\ttrain-auc:0.823023\n",
      "[492]\ttrain-auc:0.823108\n",
      "[493]\ttrain-auc:0.823187\n",
      "[494]\ttrain-auc:0.823358\n",
      "[495]\ttrain-auc:0.823427\n",
      "[496]\ttrain-auc:0.823514\n",
      "[497]\ttrain-auc:0.823642\n",
      "[498]\ttrain-auc:0.823691\n",
      "[499]\ttrain-auc:0.823792\n"
     ]
    }
   ],
   "source": [
    "df_train = train_df\r\n",
    "df_test = test_df\r\n",
    "target1 = train_df['favorite']\r\n",
    "target2 = train_df['purchase']\r\n",
    "categorical_features = ['seller','user_id','Product_id']\r\n",
    "for col in categorical_features:\r\n",
    "        train_df[col] = train_df[col].astype('category')\r\n",
    "        test_df[col] = test_df[col].astype('category')\r\n",
    "df_train_columns = df_train.columns.tolist()\r\n",
    "# 3个object特征\r\n",
    "df_train_columns.remove('seller')\r\n",
    "df_train_columns.remove('user_id')\r\n",
    "df_train_columns.remove('Product_id')\r\n",
    "# 2个target\r\n",
    "df_train_columns.remove('favorite')\r\n",
    "df_train_columns.remove('purchase')\r\n",
    "\r\n",
    "df_train_columns.remove('seller_procduction')\r\n",
    "df_train_columns.remove('seller_user')\r\n",
    "df_train_columns.remove('user_procduction')\r\n",
    "df_train_columns.remove('seller_user_procduction')\r\n",
    "df_train_columns.remove('production_action')\r\n",
    "# df_train_columns.remove('seller_user_count')\r\n",
    "# df_train_columns.remove(\"mean_act_count2\")\r\n",
    "\r\n",
    "# df_train_columns.remove(\"con_act_max2\")\r\n",
    "df_train_columns.remove('action_type')\r\n",
    "# df_train_columns.remove(\"seller_count\")\r\n",
    "df_train_columns1 = df_train_columns.copy()\r\n",
    "# df_train_columns1.append('favorite')\r\n",
    "\r\n",
    "# df_train_columns.remove('mean(purchase)')\r\n",
    "df_train_columns.remove('mean(purchase)_seller')\r\n",
    "# df_train_columns.remove('mean(purchase)_product')\r\n",
    "\r\n",
    "df_train_columns1.remove('seller_user_')\r\n",
    "df_train_columns1.remove('user_seller')\r\n",
    "df_train_columns1.remove('seller_product')\r\n",
    "df_train_columns1.remove('product_seller')\r\n",
    "df_train_columns1.remove('product_user')\r\n",
    "df_train_columns1.remove('user_product')\r\n",
    "\r\n",
    "df_train_columns1.remove('mean(favorite)')\r\n",
    "df_train_columns1.remove('mean(favorite)_seller')\r\n",
    "# df_train_columns1.remove('mean_favorite_day')\r\n",
    "# df_train_columns1.remove('mean(purchase)_seller')\r\n",
    "# df_train_columns1.remove('mean(purchase)_product')\r\n",
    "\r\n",
    "# df_train_columns1 = ['UserInfo_40','UserInfo_197','UserInfo_160','UserInfo_261','UserInfo_8','UserInfo_3','UserInfo_12','UserInfo_127','UserInfo_227']\r\n",
    "target_favorite = train_df['favorite']\r\n",
    "target_purchase = train_df['purchase']\r\n",
    "\r\n",
    "#df_train_columns = xgb_fea_select(df_train.iloc[:][df_train_columns], df_train['favorite'], df_train_columns)\r\n",
    "\r\n",
    "df_train_columns1 = xgb_fea_select(df_train.iloc[:][df_train_columns1], df_train['purchase'], df_train_columns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "id": "5D8E5BE73C104ECA8DCFCB58F256E01D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import feature_selection\r\n",
    "# from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "# df_train = df_train.fillna(df_train.mean())\r\n",
    "# lr_selector = feature_selection.SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit(df_train[df_train_columns], df_train['favorite'])\r\n",
    "# indx = temp._get_support_mask().tolist()\r\n",
    "# scores = get_importance(temp.estimator_).tolist()\r\n",
    "# result = temp.transform(matrix).tolist()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "id": "9E7F1D0840554472AA0B3A57389D3719",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "133\n",
      "(33000, 430)\n"
     ]
    }
   ],
   "source": [
    "# temp_columns = df_train_columns.copy()\n",
    "# df_train_columns = temp_columns.copy()\n",
    "# # df_train_columns += ['user_action']\n",
    "print(len(df_train_columns)) #137\n",
    "print(len(df_train_columns1)) #144\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AA3D80FBE0D4DE382DB153CF0948BF7",
    "mdEditEnable": false
   },
   "source": [
    "# lgb_favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "id": "34C348D9CF5240DEBD741948C72DD17A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.880002\tvalid_1's auc: 0.876738\n",
      "[400]\ttraining's auc: 0.904917\tvalid_1's auc: 0.895944\n",
      "[600]\ttraining's auc: 0.92116\tvalid_1's auc: 0.901151\n",
      "Early stopping, best iteration is:\n",
      "[678]\ttraining's auc: 0.926169\tvalid_1's auc: 0.90134\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.880626\tvalid_1's auc: 0.873603\n",
      "[400]\ttraining's auc: 0.904219\tvalid_1's auc: 0.890564\n",
      "[600]\ttraining's auc: 0.921397\tvalid_1's auc: 0.897312\n",
      "[800]\ttraining's auc: 0.93437\tvalid_1's auc: 0.899149\n",
      "[1000]\ttraining's auc: 0.945192\tvalid_1's auc: 0.900034\n",
      "[1200]\ttraining's auc: 0.954434\tvalid_1's auc: 0.900435\n",
      "[1400]\ttraining's auc: 0.962192\tvalid_1's auc: 0.900553\n",
      "Early stopping, best iteration is:\n",
      "[1384]\ttraining's auc: 0.961599\tvalid_1's auc: 0.900628\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.879613\tvalid_1's auc: 0.875999\n",
      "[400]\ttraining's auc: 0.904295\tvalid_1's auc: 0.894064\n",
      "[600]\ttraining's auc: 0.92126\tvalid_1's auc: 0.900616\n",
      "[800]\ttraining's auc: 0.933856\tvalid_1's auc: 0.901939\n",
      "[1000]\ttraining's auc: 0.94433\tvalid_1's auc: 0.902558\n",
      "[1200]\ttraining's auc: 0.953405\tvalid_1's auc: 0.903266\n",
      "[1400]\ttraining's auc: 0.961321\tvalid_1's auc: 0.903941\n",
      "[1600]\ttraining's auc: 0.968308\tvalid_1's auc: 0.904705\n",
      "[1800]\ttraining's auc: 0.974402\tvalid_1's auc: 0.905374\n",
      "[2000]\ttraining's auc: 0.97943\tvalid_1's auc: 0.905779\n",
      "[2200]\ttraining's auc: 0.983665\tvalid_1's auc: 0.906549\n",
      "Early stopping, best iteration is:\n",
      "[2149]\ttraining's auc: 0.982722\tvalid_1's auc: 0.906681\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.881666\tvalid_1's auc: 0.878031\n",
      "[400]\ttraining's auc: 0.904736\tvalid_1's auc: 0.894398\n",
      "[600]\ttraining's auc: 0.920413\tvalid_1's auc: 0.900383\n",
      "[800]\ttraining's auc: 0.932355\tvalid_1's auc: 0.901307\n",
      "[1000]\ttraining's auc: 0.942851\tvalid_1's auc: 0.901679\n",
      "[1200]\ttraining's auc: 0.952386\tvalid_1's auc: 0.902526\n",
      "[1400]\ttraining's auc: 0.960551\tvalid_1's auc: 0.903077\n",
      "[1600]\ttraining's auc: 0.967559\tvalid_1's auc: 0.903464\n",
      "[1800]\ttraining's auc: 0.973663\tvalid_1's auc: 0.904039\n",
      "[2000]\ttraining's auc: 0.97869\tvalid_1's auc: 0.904491\n",
      "[2200]\ttraining's auc: 0.98315\tvalid_1's auc: 0.904947\n",
      "[2400]\ttraining's auc: 0.986615\tvalid_1's auc: 0.905776\n",
      "[2600]\ttraining's auc: 0.989605\tvalid_1's auc: 0.906202\n",
      "[2800]\ttraining's auc: 0.992055\tvalid_1's auc: 0.906928\n",
      "Early stopping, best iteration is:\n",
      "[2882]\ttraining's auc: 0.992837\tvalid_1's auc: 0.907498\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's auc: 0.881378\tvalid_1's auc: 0.870247\n",
      "[400]\ttraining's auc: 0.905251\tvalid_1's auc: 0.890663\n",
      "[600]\ttraining's auc: 0.921379\tvalid_1's auc: 0.89644\n",
      "[800]\ttraining's auc: 0.933619\tvalid_1's auc: 0.897248\n",
      "[1000]\ttraining's auc: 0.944015\tvalid_1's auc: 0.897507\n",
      "Early stopping, best iteration is:\n",
      "[934]\ttraining's auc: 0.940785\tvalid_1's auc: 0.897744\n",
      "0.9025881551930777\n"
     ]
    }
   ],
   "source": [
    "# favorite\r\n",
    "param = {'num_leaves': 31,\r\n",
    "         'min_data_in_leaf': 30, \r\n",
    "         'objective':'binary',\r\n",
    "         'max_depth': -1,\r\n",
    "         'learning_rate': 0.01,\r\n",
    "         \"min_child_samples\": 20,\r\n",
    "         \"boosting\": \"gbdt\",\r\n",
    "         \"feature_fraction\": 0.9,\r\n",
    "         \"bagging_freq\": 1,\r\n",
    "         \"bagging_fraction\": 0.9 ,\r\n",
    "         \"bagging_seed\": 11,\r\n",
    "         \"metric\": 'auc',\r\n",
    "         \"lambda_l1\": 0.1,\r\n",
    "         \"verbose\": -1,\r\n",
    "         \"nthread\": 4,\r\n",
    "         \"random_state\": 666}\r\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=666)\r\n",
    "oof = np.zeros(len(df_train))\r\n",
    "predictions = np.zeros(len(df_test))\r\n",
    "# feature_importance_df = pd.DataFrame()\r\n",
    "feature_importance_favorite = pd.DataFrame(df_train_columns, columns = ['feature'])\r\n",
    "\r\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['favorite'].values)):\r\n",
    "    print(\"fold {}\".format(fold_))\r\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target1.iloc[trn_idx])#, categorical_feature=categorical_features)\r\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target1.iloc[val_idx])#, categorical_feature=categorical_features)\r\n",
    "\r\n",
    "    num_round = 10000\r\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)\r\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\r\n",
    "    feature_importance_favorite[f\"importance_fold{fold_}\"] = clf.feature_importance()\r\n",
    "    # fold_importance_df = pd.DataFrame()\r\n",
    "    # fold_importance_df[\"Feature\"] = df_train_columns\r\n",
    "    # fold_importance_df[\"importance\"] = clf.feature_importance()\r\n",
    "    # fold_importance_df[\"fold\"] = fold_ + 1\r\n",
    "    # feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\r\n",
    "    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\r\n",
    "\r\n",
    "print(roc_auc_score(target1.values, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "id": "07455F3564F743139621E184C3F49AC5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#0.9035534899726219 线上638\n",
    "#0.8999157474028441 线上640\n",
    "#0.8750573848315113 线上637\n",
    "#0.7044681140320659 线上620\n",
    "#0.700491297865058 线上619\n",
    "#0.6993089706468049?\n",
    "#0.7005724298591971 线上618 6+12+6+time_from_start*6\n",
    "#0.6949741785758629 线上617 6+12+6\n",
    "#0.6891717304314795 线上616 6+12 action_type\n",
    "#0.6783940168424141 线上614 6个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "id": "24B68921F9F246E7A7CD219882AEA634",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feature_importance_favorite[feature_importance_favorite['feature'] == 'UserInfo_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "id": "A530665DD78644269050146A735D7AE8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feature_importance_favorite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "id": "83207354B2B546468E254B32663221A9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nums = ['UserInfo_4','UserInfo_186','UserInfo_229','UserInfo_169','UserInfo_121']\r\n",
    "# # nums = ['ProductInfo_87','ProductInfo_37','ProductInfo_133','ProductInfo_2','ProductInfo_96']\r\n",
    "# for i in range(len(nums)):\r\n",
    "#     if nums[i] in df_train_columns:\r\n",
    "#         print(i)\r\n",
    "# # # [ProductInfo_87,ProductInfo_37,ProductInfo_133,ProductInfo_2,ProductInfo_96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "id": "BD35EB4585674BA88D6D71EFE9A59E57",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/gridfs/static_files/rt_upload/BD35EB4585674BA88D6D71EFE9A59E57/pwfc9ptwgu.png\">"
      ],
      "text/plain": [
       "<Figure size 1008x2880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# favorite特征重要度\r\n",
    "feature_importance_favorite['importance'] = feature_importance_favorite.iloc[:,1:].mean(axis = 1)\r\n",
    "plt.figure(figsize=(14,40))\r\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_favorite.sort_values(by=\"importance\", ascending=False))\r\n",
    "plt.title('LightGBM  Features (avg over folds)')\r\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD44AA53851244C0847213B86E338C9C",
    "mdEditEnable": false
   },
   "source": [
    "# lgb_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "id": "D41A66CD0F49497D8F1784685E747737",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_test['favorite'] = predictions\n",
    "# df_test['favorite'] = df_test['favorite'].apply(lambda x : 1 if x>0.5 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "id": "464B19DDEF9E4AFD82AD3DFB1032C912",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.806046\tvalid_1's auc: 0.791778\n",
      "[200]\ttraining's auc: 0.825885\tvalid_1's auc: 0.797714\n",
      "[300]\ttraining's auc: 0.856198\tvalid_1's auc: 0.808585\n",
      "[400]\ttraining's auc: 0.883621\tvalid_1's auc: 0.815789\n",
      "[500]\ttraining's auc: 0.90853\tvalid_1's auc: 0.819658\n",
      "[600]\ttraining's auc: 0.928796\tvalid_1's auc: 0.82237\n",
      "[700]\ttraining's auc: 0.944994\tvalid_1's auc: 0.824262\n",
      "[800]\ttraining's auc: 0.957138\tvalid_1's auc: 0.825248\n",
      "[900]\ttraining's auc: 0.967532\tvalid_1's auc: 0.826007\n",
      "[1000]\ttraining's auc: 0.975016\tvalid_1's auc: 0.826685\n",
      "[1100]\ttraining's auc: 0.981088\tvalid_1's auc: 0.827496\n",
      "[1200]\ttraining's auc: 0.985754\tvalid_1's auc: 0.828028\n",
      "[1300]\ttraining's auc: 0.989453\tvalid_1's auc: 0.827908\n",
      "[1400]\ttraining's auc: 0.992201\tvalid_1's auc: 0.827799\n",
      "Early stopping, best iteration is:\n",
      "[1355]\ttraining's auc: 0.99106\tvalid_1's auc: 0.828148\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.804094\tvalid_1's auc: 0.773219\n",
      "[200]\ttraining's auc: 0.82878\tvalid_1's auc: 0.783318\n",
      "[300]\ttraining's auc: 0.857668\tvalid_1's auc: 0.79448\n",
      "[400]\ttraining's auc: 0.885755\tvalid_1's auc: 0.801091\n",
      "[500]\ttraining's auc: 0.9103\tvalid_1's auc: 0.805536\n",
      "[600]\ttraining's auc: 0.930077\tvalid_1's auc: 0.808779\n",
      "[700]\ttraining's auc: 0.946028\tvalid_1's auc: 0.810254\n",
      "[800]\ttraining's auc: 0.958719\tvalid_1's auc: 0.811301\n",
      "[900]\ttraining's auc: 0.968603\tvalid_1's auc: 0.813131\n",
      "[1000]\ttraining's auc: 0.976331\tvalid_1's auc: 0.813913\n",
      "[1100]\ttraining's auc: 0.982141\tvalid_1's auc: 0.814616\n",
      "[1200]\ttraining's auc: 0.986624\tvalid_1's auc: 0.815136\n",
      "[1300]\ttraining's auc: 0.990208\tvalid_1's auc: 0.815219\n",
      "[1400]\ttraining's auc: 0.992863\tvalid_1's auc: 0.815601\n",
      "[1500]\ttraining's auc: 0.994854\tvalid_1's auc: 0.815612\n",
      "Early stopping, best iteration is:\n",
      "[1433]\ttraining's auc: 0.993565\tvalid_1's auc: 0.815899\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.804133\tvalid_1's auc: 0.775924\n",
      "[200]\ttraining's auc: 0.824359\tvalid_1's auc: 0.784944\n",
      "[300]\ttraining's auc: 0.853555\tvalid_1's auc: 0.799031\n",
      "[400]\ttraining's auc: 0.882497\tvalid_1's auc: 0.805875\n",
      "[500]\ttraining's auc: 0.907149\tvalid_1's auc: 0.810328\n",
      "[600]\ttraining's auc: 0.928184\tvalid_1's auc: 0.812272\n",
      "[700]\ttraining's auc: 0.944423\tvalid_1's auc: 0.81403\n",
      "[800]\ttraining's auc: 0.957351\tvalid_1's auc: 0.815098\n",
      "[900]\ttraining's auc: 0.967764\tvalid_1's auc: 0.815339\n",
      "[1000]\ttraining's auc: 0.975981\tvalid_1's auc: 0.816012\n",
      "[1100]\ttraining's auc: 0.982046\tvalid_1's auc: 0.816334\n",
      "[1200]\ttraining's auc: 0.986712\tvalid_1's auc: 0.816722\n",
      "Early stopping, best iteration is:\n",
      "[1186]\ttraining's auc: 0.986066\tvalid_1's auc: 0.816739\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.802914\tvalid_1's auc: 0.780051\n",
      "[200]\ttraining's auc: 0.826176\tvalid_1's auc: 0.787606\n",
      "[300]\ttraining's auc: 0.856107\tvalid_1's auc: 0.797464\n",
      "[400]\ttraining's auc: 0.883497\tvalid_1's auc: 0.801804\n",
      "[500]\ttraining's auc: 0.90701\tvalid_1's auc: 0.806691\n",
      "[600]\ttraining's auc: 0.926845\tvalid_1's auc: 0.808604\n",
      "[700]\ttraining's auc: 0.94295\tvalid_1's auc: 0.811251\n",
      "[800]\ttraining's auc: 0.955827\tvalid_1's auc: 0.811571\n",
      "[900]\ttraining's auc: 0.965619\tvalid_1's auc: 0.812604\n",
      "[1000]\ttraining's auc: 0.97378\tvalid_1's auc: 0.812789\n",
      "Early stopping, best iteration is:\n",
      "[994]\ttraining's auc: 0.973302\tvalid_1's auc: 0.812896\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.807127\tvalid_1's auc: 0.766183\n",
      "[200]\ttraining's auc: 0.829474\tvalid_1's auc: 0.776628\n",
      "[300]\ttraining's auc: 0.857878\tvalid_1's auc: 0.786667\n",
      "[400]\ttraining's auc: 0.886642\tvalid_1's auc: 0.791797\n",
      "[500]\ttraining's auc: 0.909984\tvalid_1's auc: 0.796688\n",
      "[600]\ttraining's auc: 0.929134\tvalid_1's auc: 0.80082\n",
      "[700]\ttraining's auc: 0.944522\tvalid_1's auc: 0.803212\n",
      "[800]\ttraining's auc: 0.95732\tvalid_1's auc: 0.804314\n",
      "[900]\ttraining's auc: 0.967075\tvalid_1's auc: 0.80455\n",
      "[1000]\ttraining's auc: 0.97501\tvalid_1's auc: 0.804619\n",
      "[1100]\ttraining's auc: 0.980987\tvalid_1's auc: 0.805142\n",
      "[1200]\ttraining's auc: 0.985481\tvalid_1's auc: 0.805587\n",
      "[1300]\ttraining's auc: 0.988978\tvalid_1's auc: 0.806245\n",
      "[1400]\ttraining's auc: 0.991755\tvalid_1's auc: 0.807108\n",
      "[1500]\ttraining's auc: 0.993894\tvalid_1's auc: 0.807244\n",
      "[1600]\ttraining's auc: 0.995514\tvalid_1's auc: 0.807601\n",
      "[1700]\ttraining's auc: 0.996674\tvalid_1's auc: 0.807759\n",
      "[1800]\ttraining's auc: 0.997622\tvalid_1's auc: 0.807945\n",
      "Early stopping, best iteration is:\n",
      "[1734]\ttraining's auc: 0.997042\tvalid_1's auc: 0.808147\n",
      "cv score for valid is:  0.8157660468284476\n"
     ]
    }
   ],
   "source": [
    "# purchase\r\n",
    "param = {'num_leaves': 31,\r\n",
    "         'min_data_in_leaf': 30, \r\n",
    "         'objective':'binary',\r\n",
    "         'max_depth': -1,\r\n",
    "         'learning_rate': 0.01,\r\n",
    "         \"min_child_samples\": 20,\r\n",
    "         \"boosting\": \"gbdt\",\r\n",
    "         \"feature_fraction\": 0.9,\r\n",
    "         \"bagging_freq\": 1,\r\n",
    "         \"bagging_fraction\": 0.9 ,\r\n",
    "         \"bagging_seed\": 11,\r\n",
    "         \"metric\": 'auc',\r\n",
    "         \"lambda_l1\": 0.1,\r\n",
    "         \"verbosity\": -1,\r\n",
    "         \"nthread\": 4,\r\n",
    "         \"random_state\": 666}\r\n",
    "oof = np.zeros(len(df_train))\r\n",
    "predictions1 = np.zeros(len(df_test))\r\n",
    "# feature_importance_df1 = pd.DataFrame()\r\n",
    "df_train_columns1 = df_train_columns1\r\n",
    "feature_importance_purchase = pd.DataFrame(df_train_columns1, columns = ['feature'])\r\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['purchase'].values)):\r\n",
    "    print(\"fold {}\".format(fold_))\r\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns1], label=target2.iloc[trn_idx])#, categorical_feature=categorical_feature)\r\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns1], label=target2.iloc[val_idx])#, categorical_feature=categorical_feature)\r\n",
    "\r\n",
    "    num_round = 10000\r\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\r\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns1], num_iteration=clf.best_iteration)\r\n",
    "    feature_importance_purchase[f\"importance_fold{fold_}\"] = clf.feature_importance()\r\n",
    "    # fold_importance_df1 = pd.DataFrame()\r\n",
    "    # fold_importance_df1[\"Feature\"] = df_train_columns1\r\n",
    "    # fold_importance_df1[\"importance\"] = clf.feature_importance()\r\n",
    "    # fold_importance_df1[\"fold\"] = fold_ + 1\r\n",
    "    # feature_importance_df1 = pd.concat([feature_importance_df, fold_importance_df], axis=0)\r\n",
    "    predictions1 += clf.predict(df_test[df_train_columns1], num_iteration=clf.best_iteration) / folds.n_splits\r\n",
    "print('cv score for valid is: ', roc_auc_score(target2,oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "id": "8688576383474C2F802964123C8248C6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#0.8150047543877149 640\n",
    "#0.8528995707864748 625\n",
    "#0.7253596621228516 620 product_per_day\n",
    "#0.7245031815417257 618 调整后action_type\n",
    "#0.7264163278976133 619\n",
    "\n",
    "#0.7241813760375115 6+12\n",
    "#0.7249323746494951 线上614 6个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "id": "714A7A9AD77C4A1682CBFDF59A383F04",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/gridfs/static_files/rt_upload/714A7A9AD77C4A1682CBFDF59A383F04/pwffe8viof.png\">"
      ],
      "text/plain": [
       "<Figure size 1008x2880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# favorite特征重要度\r\n",
    "feature_importance_purchase['importance'] = feature_importance_purchase.iloc[:,1:].mean(axis = 1)\r\n",
    "plt.figure(figsize=(14,40))\r\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_purchase.sort_values(by=\"importance\", ascending=False))\r\n",
    "plt.title('LightGBM  Features (avg over folds)')\r\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4DA5AF38B18484B87ECE797F4FF20E3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9806D91DE0248A19BE0C4FF5DE5B849"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89E300A251134D7FAF05FB7E0351F9BC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67B6DC2D3BC84C44A268AB749BD00A09",
    "mdEditEnable": false
   },
   "source": [
    "# xgb_favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "id": "5E449A8219E84A418CE274D991BBDE86",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.01, \r\n",
    "              'max_depth': 10, \r\n",
    "              'subsample': 0.8, \r\n",
    "              'colsample_bytree': 0.8, \r\n",
    "              'objective': 'binary:logistic', \r\n",
    "              'eval_metric': 'auc', \r\n",
    "              'silent': True, \r\n",
    "              'nthread': 4}#xgb的参数，可以自己改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "id": "E6BA92132E4B4F47B14312A91B38C6A9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n0\n",
      "[0]\ttrain-auc:0.763047\tvalid_data-auc:0.727522\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.926909\tvalid_data-auc:0.882652\n",
      "[200]\ttrain-auc:0.942527\tvalid_data-auc:0.889866\n",
      "[300]\ttrain-auc:0.95879\tvalid_data-auc:0.90106\n",
      "[400]\ttrain-auc:0.969719\tvalid_data-auc:0.906394\n",
      "[500]\ttrain-auc:0.979397\tvalid_data-auc:0.909429\n",
      "[600]\ttrain-auc:0.987587\tvalid_data-auc:0.911055\n",
      "[700]\ttrain-auc:0.992935\tvalid_data-auc:0.911899\n",
      "[800]\ttrain-auc:0.996302\tvalid_data-auc:0.912307\n",
      "[900]\ttrain-auc:0.998152\tvalid_data-auc:0.913435\n",
      "[1000]\ttrain-auc:0.999092\tvalid_data-auc:0.913846\n",
      "[1100]\ttrain-auc:0.999616\tvalid_data-auc:0.91399\n",
      "[1200]\ttrain-auc:0.999811\tvalid_data-auc:0.914994\n",
      "[1300]\ttrain-auc:0.999919\tvalid_data-auc:0.915215\n",
      "[1400]\ttrain-auc:0.999968\tvalid_data-auc:0.915388\n",
      "[1500]\ttrain-auc:0.999986\tvalid_data-auc:0.915958\n",
      "[1600]\ttrain-auc:0.999994\tvalid_data-auc:0.916445\n",
      "[1700]\ttrain-auc:0.999998\tvalid_data-auc:0.916865\n",
      "[1800]\ttrain-auc:0.999999\tvalid_data-auc:0.917242\n",
      "[1900]\ttrain-auc:1\tvalid_data-auc:0.917472\n",
      "[2000]\ttrain-auc:1\tvalid_data-auc:0.917962\n",
      "[2100]\ttrain-auc:1\tvalid_data-auc:0.91829\n",
      "[2200]\ttrain-auc:1\tvalid_data-auc:0.918522\n",
      "[2300]\ttrain-auc:1\tvalid_data-auc:0.918764\n",
      "[2400]\ttrain-auc:1\tvalid_data-auc:0.918832\n",
      "[2500]\ttrain-auc:1\tvalid_data-auc:0.919285\n",
      "[2600]\ttrain-auc:1\tvalid_data-auc:0.919311\n",
      "[2700]\ttrain-auc:1\tvalid_data-auc:0.919542\n",
      "[2800]\ttrain-auc:1\tvalid_data-auc:0.919663\n",
      "[2900]\ttrain-auc:1\tvalid_data-auc:0.919673\n",
      "Stopping. Best iteration:\n",
      "[2790]\ttrain-auc:1\tvalid_data-auc:0.919747\n",
      "\n",
      "fold n1\n",
      "[0]\ttrain-auc:0.76366\tvalid_data-auc:0.723626\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.92467\tvalid_data-auc:0.875694\n",
      "[200]\ttrain-auc:0.943628\tvalid_data-auc:0.885967\n",
      "[300]\ttrain-auc:0.959877\tvalid_data-auc:0.895664\n",
      "[400]\ttrain-auc:0.971143\tvalid_data-auc:0.901879\n",
      "[500]\ttrain-auc:0.979688\tvalid_data-auc:0.904959\n",
      "[600]\ttrain-auc:0.987805\tvalid_data-auc:0.906557\n",
      "[700]\ttrain-auc:0.993186\tvalid_data-auc:0.908012\n",
      "[800]\ttrain-auc:0.996687\tvalid_data-auc:0.908742\n",
      "[900]\ttrain-auc:0.998408\tvalid_data-auc:0.909329\n",
      "[1000]\ttrain-auc:0.999229\tvalid_data-auc:0.909809\n",
      "[1100]\ttrain-auc:0.999681\tvalid_data-auc:0.910148\n",
      "[1200]\ttrain-auc:0.999864\tvalid_data-auc:0.910541\n",
      "[1300]\ttrain-auc:0.999944\tvalid_data-auc:0.911023\n",
      "[1400]\ttrain-auc:0.999975\tvalid_data-auc:0.911878\n",
      "[1500]\ttrain-auc:0.999989\tvalid_data-auc:0.912279\n",
      "[1600]\ttrain-auc:0.999996\tvalid_data-auc:0.9127\n",
      "[1700]\ttrain-auc:0.999999\tvalid_data-auc:0.913126\n",
      "[1800]\ttrain-auc:1\tvalid_data-auc:0.913811\n",
      "[1900]\ttrain-auc:1\tvalid_data-auc:0.913866\n",
      "[2000]\ttrain-auc:1\tvalid_data-auc:0.914216\n",
      "[2100]\ttrain-auc:1\tvalid_data-auc:0.914286\n",
      "[2200]\ttrain-auc:1\tvalid_data-auc:0.91466\n",
      "[2300]\ttrain-auc:1\tvalid_data-auc:0.914963\n",
      "[2400]\ttrain-auc:1\tvalid_data-auc:0.915004\n",
      "[2500]\ttrain-auc:1\tvalid_data-auc:0.915175\n",
      "[2600]\ttrain-auc:1\tvalid_data-auc:0.915246\n",
      "[2700]\ttrain-auc:1\tvalid_data-auc:0.915347\n",
      "[2800]\ttrain-auc:1\tvalid_data-auc:0.915554\n",
      "[2900]\ttrain-auc:1\tvalid_data-auc:0.915596\n",
      "[3000]\ttrain-auc:1\tvalid_data-auc:0.915672\n",
      "[3100]\ttrain-auc:1\tvalid_data-auc:0.915752\n",
      "[3200]\ttrain-auc:1\tvalid_data-auc:0.915878\n",
      "[3300]\ttrain-auc:1\tvalid_data-auc:0.915802\n",
      "[3400]\ttrain-auc:1\tvalid_data-auc:0.91607\n",
      "[3500]\ttrain-auc:1\tvalid_data-auc:0.916219\n",
      "[3600]\ttrain-auc:1\tvalid_data-auc:0.91613\n",
      "[3700]\ttrain-auc:1\tvalid_data-auc:0.916156\n",
      "Stopping. Best iteration:\n",
      "[3512]\ttrain-auc:1\tvalid_data-auc:0.916279\n",
      "\n",
      "fold n2\n",
      "[0]\ttrain-auc:0.753627\tvalid_data-auc:0.70392\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.925386\tvalid_data-auc:0.881315\n",
      "[200]\ttrain-auc:0.942542\tvalid_data-auc:0.888754\n",
      "[300]\ttrain-auc:0.958725\tvalid_data-auc:0.899043\n",
      "[400]\ttrain-auc:0.968182\tvalid_data-auc:0.904045\n",
      "[500]\ttrain-auc:0.976923\tvalid_data-auc:0.906808\n",
      "[600]\ttrain-auc:0.985282\tvalid_data-auc:0.909048\n",
      "[700]\ttrain-auc:0.991383\tvalid_data-auc:0.909997\n",
      "[800]\ttrain-auc:0.995752\tvalid_data-auc:0.910502\n",
      "[900]\ttrain-auc:0.997883\tvalid_data-auc:0.910795\n",
      "[1000]\ttrain-auc:0.998962\tvalid_data-auc:0.91141\n",
      "[1100]\ttrain-auc:0.999515\tvalid_data-auc:0.911705\n",
      "[1200]\ttrain-auc:0.999767\tvalid_data-auc:0.912714\n",
      "[1300]\ttrain-auc:0.999896\tvalid_data-auc:0.913012\n",
      "[1400]\ttrain-auc:0.999954\tvalid_data-auc:0.913518\n",
      "[1500]\ttrain-auc:0.999979\tvalid_data-auc:0.913966\n",
      "[1600]\ttrain-auc:0.99999\tvalid_data-auc:0.91439\n",
      "[1700]\ttrain-auc:0.999995\tvalid_data-auc:0.914696\n",
      "[1800]\ttrain-auc:0.999998\tvalid_data-auc:0.915105\n",
      "[1900]\ttrain-auc:0.999999\tvalid_data-auc:0.915571\n",
      "[2000]\ttrain-auc:1\tvalid_data-auc:0.916102\n",
      "[2100]\ttrain-auc:1\tvalid_data-auc:0.916348\n",
      "[2200]\ttrain-auc:1\tvalid_data-auc:0.916642\n",
      "[2300]\ttrain-auc:1\tvalid_data-auc:0.917003\n",
      "[2400]\ttrain-auc:1\tvalid_data-auc:0.917308\n",
      "[2500]\ttrain-auc:1\tvalid_data-auc:0.917382\n",
      "[2600]\ttrain-auc:1\tvalid_data-auc:0.917326\n",
      "[2700]\ttrain-auc:1\tvalid_data-auc:0.917366\n",
      "[2800]\ttrain-auc:1\tvalid_data-auc:0.917649\n",
      "[2900]\ttrain-auc:1\tvalid_data-auc:0.917749\n",
      "[3000]\ttrain-auc:1\tvalid_data-auc:0.917906\n",
      "[3100]\ttrain-auc:1\tvalid_data-auc:0.917902\n",
      "[3200]\ttrain-auc:1\tvalid_data-auc:0.918105\n",
      "[3300]\ttrain-auc:1\tvalid_data-auc:0.917923\n",
      "Stopping. Best iteration:\n",
      "[3196]\ttrain-auc:1\tvalid_data-auc:0.918105\n",
      "\n",
      "fold n3\n",
      "[0]\ttrain-auc:0.79217\tvalid_data-auc:0.743051\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.927432\tvalid_data-auc:0.882294\n",
      "[200]\ttrain-auc:0.944084\tvalid_data-auc:0.890762\n",
      "[300]\ttrain-auc:0.959687\tvalid_data-auc:0.901957\n",
      "[400]\ttrain-auc:0.969162\tvalid_data-auc:0.906637\n",
      "[500]\ttrain-auc:0.977401\tvalid_data-auc:0.909599\n",
      "[600]\ttrain-auc:0.985244\tvalid_data-auc:0.911144\n",
      "[700]\ttrain-auc:0.991242\tvalid_data-auc:0.911576\n",
      "[800]\ttrain-auc:0.995288\tvalid_data-auc:0.912636\n",
      "[900]\ttrain-auc:0.997341\tvalid_data-auc:0.913416\n",
      "[1000]\ttrain-auc:0.998611\tvalid_data-auc:0.913904\n",
      "[1100]\ttrain-auc:0.999285\tvalid_data-auc:0.91438\n",
      "[1200]\ttrain-auc:0.999657\tvalid_data-auc:0.915141\n",
      "[1300]\ttrain-auc:0.999846\tvalid_data-auc:0.915501\n",
      "[1400]\ttrain-auc:0.999922\tvalid_data-auc:0.916535\n",
      "[1500]\ttrain-auc:0.999961\tvalid_data-auc:0.916915\n",
      "[1600]\ttrain-auc:0.999983\tvalid_data-auc:0.916983\n",
      "[1700]\ttrain-auc:0.999993\tvalid_data-auc:0.917203\n",
      "[1800]\ttrain-auc:0.999997\tvalid_data-auc:0.917684\n",
      "[1900]\ttrain-auc:0.999999\tvalid_data-auc:0.918331\n",
      "[2000]\ttrain-auc:1\tvalid_data-auc:0.918878\n",
      "[2100]\ttrain-auc:1\tvalid_data-auc:0.919571\n",
      "[2200]\ttrain-auc:1\tvalid_data-auc:0.919901\n",
      "[2300]\ttrain-auc:1\tvalid_data-auc:0.919939\n",
      "[2400]\ttrain-auc:1\tvalid_data-auc:0.920207\n",
      "[2500]\ttrain-auc:1\tvalid_data-auc:0.920377\n",
      "[2600]\ttrain-auc:1\tvalid_data-auc:0.920391\n",
      "[2700]\ttrain-auc:1\tvalid_data-auc:0.920441\n",
      "Stopping. Best iteration:\n",
      "[2562]\ttrain-auc:1\tvalid_data-auc:0.92061\n",
      "\n",
      "fold n4\n",
      "[0]\ttrain-auc:0.783524\tvalid_data-auc:0.722515\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.926051\tvalid_data-auc:0.877255\n",
      "[200]\ttrain-auc:0.944534\tvalid_data-auc:0.886005\n",
      "[300]\ttrain-auc:0.959576\tvalid_data-auc:0.89554\n",
      "[400]\ttrain-auc:0.968767\tvalid_data-auc:0.899847\n",
      "[500]\ttrain-auc:0.977258\tvalid_data-auc:0.90273\n",
      "[600]\ttrain-auc:0.985054\tvalid_data-auc:0.904389\n",
      "[700]\ttrain-auc:0.991376\tvalid_data-auc:0.905209\n",
      "[800]\ttrain-auc:0.995044\tvalid_data-auc:0.905806\n",
      "[900]\ttrain-auc:0.997288\tvalid_data-auc:0.906578\n",
      "[1000]\ttrain-auc:0.998689\tvalid_data-auc:0.907242\n",
      "[1100]\ttrain-auc:0.999399\tvalid_data-auc:0.90757\n",
      "[1200]\ttrain-auc:0.999692\tvalid_data-auc:0.908278\n",
      "[1300]\ttrain-auc:0.999875\tvalid_data-auc:0.908941\n",
      "[1400]\ttrain-auc:0.999948\tvalid_data-auc:0.909731\n",
      "[1500]\ttrain-auc:0.999978\tvalid_data-auc:0.91061\n",
      "[1600]\ttrain-auc:0.999992\tvalid_data-auc:0.911371\n",
      "[1700]\ttrain-auc:0.999997\tvalid_data-auc:0.911854\n",
      "[1800]\ttrain-auc:0.999999\tvalid_data-auc:0.912275\n",
      "[1900]\ttrain-auc:1\tvalid_data-auc:0.912985\n",
      "[2000]\ttrain-auc:1\tvalid_data-auc:0.91357\n",
      "[2100]\ttrain-auc:1\tvalid_data-auc:0.914054\n",
      "[2200]\ttrain-auc:1\tvalid_data-auc:0.914585\n",
      "[2300]\ttrain-auc:1\tvalid_data-auc:0.914937\n",
      "[2400]\ttrain-auc:1\tvalid_data-auc:0.915224\n",
      "[2500]\ttrain-auc:1\tvalid_data-auc:0.915423\n",
      "[2600]\ttrain-auc:1\tvalid_data-auc:0.915658\n",
      "[2700]\ttrain-auc:1\tvalid_data-auc:0.91597\n",
      "[2800]\ttrain-auc:1\tvalid_data-auc:0.916169\n",
      "[2900]\ttrain-auc:1\tvalid_data-auc:0.916162\n",
      "[3000]\ttrain-auc:1\tvalid_data-auc:0.916384\n",
      "[3100]\ttrain-auc:1\tvalid_data-auc:0.916576\n",
      "[3200]\ttrain-auc:1\tvalid_data-auc:0.91663\n",
      "[3300]\ttrain-auc:1\tvalid_data-auc:0.916917\n",
      "[3400]\ttrain-auc:1\tvalid_data-auc:0.9171\n",
      "[3500]\ttrain-auc:1\tvalid_data-auc:0.917117\n",
      "[3600]\ttrain-auc:1\tvalid_data-auc:0.917182\n",
      "[3700]\ttrain-auc:1\tvalid_data-auc:0.91723\n",
      "[3800]\ttrain-auc:1\tvalid_data-auc:0.917169\n",
      "[3900]\ttrain-auc:1\tvalid_data-auc:0.917307\n",
      "[4000]\ttrain-auc:1\tvalid_data-auc:0.917371\n",
      "[4100]\ttrain-auc:1\tvalid_data-auc:0.917238\n",
      "Stopping. Best iteration:\n",
      "[3999]\ttrain-auc:1\tvalid_data-auc:0.917371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_xgb = np.zeros(len(df_train))#用于存放训练集的预测\r\n",
    "predictions_xgb = np.zeros(len(df_test))#用于存放测试集的预测\r\n",
    "\r\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['favorite'].values)):\r\n",
    "    print(\"fold n{}\".format(fold_))\r\n",
    "    trn_data = xgb.DMatrix(df_train.iloc[trn_idx][df_train_columns], target1.iloc[trn_idx])#训练集的80%\r\n",
    "    val_data = xgb.DMatrix(df_train.iloc[val_idx][df_train_columns], target1.iloc[val_idx])#训练集的20%，验证集\r\n",
    " \r\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\r\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=10000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)\r\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(df_train.iloc[val_idx][df_train_columns]), ntree_limit=clf.best_ntree_limit)#预测20%的验证集\r\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(df_test[df_train_columns]), ntree_limit=clf.best_ntree_limit) / folds.n_splits#预测测试集，并且取平均\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "id": "9293B6806E634CD2B82E4D0F6DEE93A4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.91805575\n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.8f}\".format(roc_auc_score(target1, oof_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14B2AC7478EF48419952E8CB65EEA312",
    "mdEditEnable": false
   },
   "source": [
    "# xgb_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "id": "B5B2BB8D3CC84BEF8388869553F1EC9A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n0\n",
      "[0]\ttrain-auc:0.664024\tvalid_data-auc:0.547299\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.968895\tvalid_data-auc:0.573016\n",
      "[200]\ttrain-auc:0.991192\tvalid_data-auc:0.571091\n",
      "[300]\ttrain-auc:0.997447\tvalid_data-auc:0.570898\n",
      "Stopping. Best iteration:\n",
      "[102]\ttrain-auc:0.96996\tvalid_data-auc:0.573988\n",
      "\n",
      "fold n1\n",
      "[0]\ttrain-auc:0.666057\tvalid_data-auc:0.540996\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.976525\tvalid_data-auc:0.561611\n",
      "[200]\ttrain-auc:0.994153\tvalid_data-auc:0.559108\n",
      "Stopping. Best iteration:\n",
      "[31]\ttrain-auc:0.924758\tvalid_data-auc:0.56571\n",
      "\n",
      "fold n2\n",
      "[0]\ttrain-auc:0.67675\tvalid_data-auc:0.522764\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.974608\tvalid_data-auc:0.553035\n",
      "[200]\ttrain-auc:0.990373\tvalid_data-auc:0.554628\n",
      "[300]\ttrain-auc:0.996537\tvalid_data-auc:0.55455\n",
      "[400]\ttrain-auc:0.998839\tvalid_data-auc:0.553427\n",
      "Stopping. Best iteration:\n",
      "[206]\ttrain-auc:0.99082\tvalid_data-auc:0.555348\n",
      "\n",
      "fold n3\n",
      "[0]\ttrain-auc:0.649501\tvalid_data-auc:0.529932\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.964934\tvalid_data-auc:0.563132\n",
      "[200]\ttrain-auc:0.989527\tvalid_data-auc:0.561348\n",
      "Stopping. Best iteration:\n",
      "[40]\ttrain-auc:0.933629\tvalid_data-auc:0.569299\n",
      "\n",
      "fold n4\n",
      "[0]\ttrain-auc:0.66164\tvalid_data-auc:0.533201\n",
      "Multiple eval metrics have been passed: 'valid_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-auc hasn't improved in 200 rounds.\n",
      "[100]\ttrain-auc:0.976789\tvalid_data-auc:0.567356\n",
      "[200]\ttrain-auc:0.993062\tvalid_data-auc:0.562923\n",
      "Stopping. Best iteration:\n",
      "[88]\ttrain-auc:0.974632\tvalid_data-auc:0.567798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_xgb = np.zeros(len(df_train))#用于存放训练集的预测\r\n",
    "predictions_xgb1 = np.zeros(len(df_test))#用于存放测试集的预测\r\n",
    "\r\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['purchase'].values)):\r\n",
    "    print(\"fold n{}\".format(fold_))\r\n",
    "    trn_data = xgb.DMatrix(df_train.iloc[trn_idx][df_train_columns1], target1.iloc[trn_idx])#训练集的80%\r\n",
    "    val_data = xgb.DMatrix(df_train.iloc[val_idx][df_train_columns1], target1.iloc[val_idx])#训练集的20%，验证集\r\n",
    " \r\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\r\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=10000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)\r\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(df_train.iloc[val_idx][df_train_columns1]), ntree_limit=clf.best_ntree_limit)#预测20%的验证集\r\n",
    "    predictions_xgb1 += clf.predict(xgb.DMatrix(df_test[df_train_columns1]), ntree_limit=clf.best_ntree_limit) / folds.n_splits#预测测试集，并且取平均\r\n",
    "    \r\n",
    "# print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, target)))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "id": "B5085B1AE5584817992936F871FE2160",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.51324584\n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.8f}\".format(roc_auc_score(target2,oof_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A41235B09774FFB8895BD3C8AFF2B48",
    "mdEditEnable": false
   },
   "source": [
    "# 提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "id": "FD59710D4EC649E1996951F5ECC5E042",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submition = pd.DataFrame(test_df, columns= ['user_id','Product_id'])\r\n",
    "submition = submition.rename(index=str, columns={\"Product_id\": \"product_id\"})\r\n",
    "submition['pred_favorite'] = predictions # \r\n",
    "submition['pred_purchase'] = predictions1 #\r\n",
    "\r\n",
    "# submition['pred_favorite'] = submition['pred_favorite'].apply(lambda x : 0.95 if x > 0.95 else x)\r\n",
    "# submition['pred_favorite'] = submition['pred_favorite'].apply(lambda x : 0.05 if x < 0.05 else x)\r\n",
    "# submition['pred_purchase'] = submition['pred_purchase'].apply(lambda x : 0.95 if x > 0.95 else x)\r\n",
    "# submition['pred_purchase'] = submition['pred_purchase'].apply(lambda x : 0.05 if x < 0.05 else x)\r\n",
    "\r\n",
    "# submition['pred_favorite'] = predictions_xgb \r\n",
    "# submition['pred_purchase'] = predictions1\r\n",
    "\r\n",
    "# submition['pred_favorite'] = (predictions*0.64+predictions_xgb*0.64)/(0.64+0.64) # \r\n",
    "# submition['pred_purchase'] = (predictions1*0.64+predictions_xgb1*0.64) /(0.64+0.64)#\r\n",
    "\r\n",
    "# submition['pred_favorite'] = predictions_xgb*0.45+ predictions*0.55 # 0.0.0.5503838727351065\r\n",
    "# submition['pred_purchase'] = predictions_xgb1*0.3+ predictions1*0.7 #.0.0.5062587993915597\r\n",
    "# submition['pred_favorite'] = submition['pred_favorite'].apply(lambda x:round(x,1))\r\n",
    "# submition['pred_purchase'] = submition['pred_purchase'].apply(lambda x:round(x,1))\r\n",
    "submition.head()\r\n",
    "submition.to_csv('/home/kesci/work/sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "id": "8D2A2FC6A3DB48948D1A78CC6B9EBC62",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# submition[submition['pred_favorite'] > 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "id": "EDE8F636F93F46828B94DFC84152CF19",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10087, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "id": "C60196422696476690296436A8D87ACE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgeek Submit Tool\n",
      "Result File: sub.csv (0.529 MiB)\n",
      "Uploaded.       \n",
      "====================\n",
      "Submit Success.\n",
      "{\"message\":\"提交成功，请等待评审完成\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./upgeek_submit_tool -file sub.csv -token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0YXNrIjoidGFzazIiLCJhdXRoVHlwZSI6InN1Ym1pdCIsInRlYW0iOiI4NjUwMDgiLCJpYXQiOjE1NjQzODQ3NjJ9.gatKmml30p6gkSILk0UkTFQcqOf1XsWcyLgpauo8aAo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62448A8F4B5F4004A7B1D65C979E0D60"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "580BD8C980BE477BB5A65A6CF3520CCB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABB4B5B74113412482A30DEB479CECBB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
